{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import molml \n",
    "from molml.features import CoulombMatrix, BagOfBonds\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from keras.layers import Input, Dense, Lambda\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import objectives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.ensemble import RandomForestRegressor as rfr\n",
    "from sklearn.metrics import r2_score,mean_squared_error, mean_absolute_error, median_absolute_error\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from keras.models import Sequential\n",
    "from keras import regularizers\n",
    "from keras.layers.convolutional import Convolution1D\n",
    "from keras.layers.core import Flatten, RepeatVector\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.layers.wrappers import TimeDistributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vae_133(train, train_vali, epoch_no = 5, encoder_dim=[100, 100, 100], decoder_dim=[100, 100, 100]):\n",
    "    \n",
    "    # Variational autoencoder\n",
    "\n",
    "    '''This script demonstrates how to build a variational autoencoder with Keras.\n",
    "    Reference: \"Auto-Encoding Variational Bayes\" https://arxiv.org/abs/1312.6114\n",
    "    The encoder_dim are the original dim, intermediate dim and latent dim respectivley\n",
    "    \n",
    "    '''   \n",
    "    #batch_size = 100\n",
    "    start = timeit.default_timer()\n",
    "\n",
    "    x = Input(shape=(encoder_dim[0],))\n",
    "    h = Dense(encoder_dim[1], activation='relu')(x)\n",
    "    h = Dense(encoder_dim[1], activation='relu')(h)\n",
    "    \n",
    "    \n",
    "    z_mean = Dense(encoder_dim[2])(h)\n",
    "    z_log_var = Dense(encoder_dim[2])(h)\n",
    "\n",
    "\n",
    "    def sampling(args):\n",
    "        z_mean, z_log_var = args\n",
    "        epsilon = K.random_normal(shape=(encoder_dim[2], ), mean=0.)\n",
    "        return z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "\n",
    "    # note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "    z = Lambda(sampling, output_shape=(encoder_dim[2],))([z_mean, z_log_var])\n",
    "\n",
    "    # we instantiate these layers separately so as to reuse them later\n",
    "    decoder_h = Dense( decoder_dim[1], activation='relu')\n",
    "    \n",
    "    decoder_mean = Dense(decoder_dim[0], activation='linear')\n",
    "    \n",
    "    h_decoded = decoder_h(z)\n",
    "    \n",
    "    x_decoded_mean = decoder_mean(h_decoded)\n",
    "\n",
    "#Wa custom loss function: the sum of a reconstruction term, and the KL divergence regularization term.\n",
    "\n",
    "    def vae_loss(x, x_decoded_mean):\n",
    "        xent_loss = encoder_dim[0] * objectives.mean_absolute_error(x, x_decoded_mean)\n",
    "#        xent_loss = encoder_dim[0] * objectives.binary_crossentropy(x, x_decoded_mean)\n",
    "        kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "        return xent_loss + kl_loss\n",
    "\n",
    "    vae = Model(x, x_decoded_mean)\n",
    "    vae.compile(optimizer='rmsprop', loss=vae_loss, metrics=['mae', 'mse'])\n",
    "    \n",
    "    history = vae.fit(train, train,\n",
    "            shuffle=True,\n",
    "            nb_epoch=epoch_no,\n",
    "            batch_size=100,\n",
    "            verbose = 1,\n",
    "           validation_data = (train_vali,train_vali))\n",
    "    \n",
    "    encoder = Model(x, z_mean)\n",
    "    \n",
    "    stop = timeit.default_timer()\n",
    "\n",
    "    print (\"The running takes %r min\" %((stop-start)/60))\n",
    "    \n",
    "    return encoder, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cnn_133(train, train_vali, epoch_no = 5, encoder_dim=[100, 100, 100], decoder_dim=[100, 100, 100]):\n",
    "    \n",
    "    # CNN autoencoder\n",
    "   \n",
    "    #batch_size = 100\n",
    "    start = timeit.default_timer()\n",
    "    x = Input(shape=(666,1 ))\n",
    "    h = Convolution1D(9, 9, activation = 'relu', name='conv_1')(x)\n",
    "    h = Flatten(name='flatten_1')(h)\n",
    "    h = Dense(encoder_dim[2], activation = 'relu', name='dense_1')(h)\n",
    "    \n",
    "    \n",
    "    z_mean = Dense(encoder_dim[2])(h)\n",
    "    z_log_var = Dense(encoder_dim[2])(h)\n",
    "\n",
    "\n",
    "    def sampling(args):\n",
    "        z_mean, z_log_var = args\n",
    "        epsilon = K.random_normal(shape=(encoder_dim[2], ), mean=0.)\n",
    "        return z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "\n",
    "    # note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "    z = Lambda(sampling, output_shape=(encoder_dim[2],))([z_mean, z_log_var])\n",
    "\n",
    "    # we instantiate these layers separately so as to reuse them later\n",
    "    decoder_h = Dense( decoder_dim[1], activation='relu')\n",
    "    \n",
    "    decoder_mean = Dense(decoder_dim[0], activation='linear')\n",
    "    \n",
    "    h_decoded = decoder_h(z)\n",
    "    \n",
    "    x_decoded_mean = decoder_mean(h_decoded)\n",
    "\n",
    "#Wa custom loss function: the sum of a reconstruction term, and the KL divergence regularization term.\n",
    "\n",
    "    def vae_loss(x, x_decoded_mean):\n",
    "        xent_loss = encoder_dim[0] * objectives.mean_absolute_error(x, x_decoded_mean)\n",
    "#        xent_loss = encoder_dim[0] * objectives.binary_crossentropy(x, x_decoded_mean)\n",
    "        kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "        return xent_loss + kl_loss\n",
    "\n",
    "    vae = Model(x, x_decoded_mean)\n",
    "    vae.compile(optimizer='rmsprop', loss=vae_loss, metrics=['mae', 'mse'])\n",
    "    \n",
    "    history = vae.fit(train, train,\n",
    "            shuffle=True,\n",
    "            nb_epoch=epoch_no,\n",
    "            batch_size=100,\n",
    "            verbose = 1,\n",
    "           validation_data = (train_vali,train_vali))\n",
    "    \n",
    "    encoder = Model(x, z_mean)\n",
    "    \n",
    "    stop = timeit.default_timer()\n",
    "\n",
    "    print (\"The running takes %r min\" %((stop-start)/60))\n",
    "    \n",
    "    return encoder, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    }
   ],
   "source": [
    "vcnn_model = MoleculeVAE()\n",
    "vcnn_model.create(latent_rep_size=10)\n",
    "vcnn_model.autoencoder.fit(train_3D_, train_3D_,\n",
    "                            shuffle=True,\n",
    "                            nb_epoch=50,\n",
    "                            batch_size=100,\n",
    "                            verbose = 1)\n",
    "\n",
    "#print (\"The running takes %r min\" %((stop-start)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MoleculeVAE():\n",
    "\n",
    "    autoencoder = None\n",
    "    \n",
    "    \n",
    "    def create(self,\n",
    "               max_length = 666,\n",
    "               latent_rep_size = 2,\n",
    "               weights_file = None):\n",
    "        channel_length = 1\n",
    "        \n",
    "        x = Input(shape=(max_length, channel_length))\n",
    "        _, z = self._buildEncoder(x, latent_rep_size, max_length)\n",
    "        self.encoder = Model(x, z)\n",
    "\n",
    "        encoded_input = Input(shape=(latent_rep_size,))\n",
    "        self.decoder = Model(\n",
    "            encoded_input,\n",
    "            self._buildDecoder(\n",
    "                encoded_input,\n",
    "                latent_rep_size,\n",
    "                max_length,\n",
    "                channel_length\n",
    "            )\n",
    "        )\n",
    "\n",
    "        x1 = Input(shape=(max_length, channel_length ))\n",
    "        vae_loss, z1 = self._buildEncoder(x1, latent_rep_size, max_length)\n",
    "        self.autoencoder = Model(\n",
    "            x1,\n",
    "            self._buildDecoder(\n",
    "                z1,\n",
    "                latent_rep_size,\n",
    "                max_length,\n",
    "                channel_length\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if weights_file:\n",
    "            self.autoencoder.load_weights(weights_file)\n",
    "            self.encoder.load_weights(weights_file, by_name = True)\n",
    "            self.decoder.load_weights(weights_file, by_name = True)\n",
    "\n",
    "        self.autoencoder.compile(optimizer = 'Adam',\n",
    "                                 loss = vae_loss,\n",
    "                                 metrics = ['mae'])\n",
    "\n",
    "    def _buildEncoder(self, x, latent_rep_size, max_length, epsilon_std = 0.01):\n",
    "        h = Convolution1D(9, 9, activation = 'relu', name='conv_1')(x)\n",
    "        h = Flatten(name='flatten_1')(h)\n",
    "        h = Dense(20, activation = 'relu', name='dense_1')(h)\n",
    "\n",
    "        def sampling(args):\n",
    "            z_mean_, z_log_var_ = args\n",
    "            batch_size = K.shape(z_mean_)[0]\n",
    "            epsilon = K.random_normal(shape=(batch_size, latent_rep_size), mean=0., std = epsilon_std)\n",
    "            return z_mean_ + K.exp(z_log_var_ / 2) * epsilon\n",
    "\n",
    "        z_mean = Dense(latent_rep_size, name='z_mean', activation = 'linear')(h)\n",
    "        z_log_var = Dense(latent_rep_size, name='z_log_var', activation = 'linear')(h)\n",
    "\n",
    "        def vae_loss(x, x_decoded_mean):\n",
    "            x = K.flatten(x)\n",
    "            x_decoded_mean = K.flatten(x_decoded_mean)\n",
    "            xent_loss = max_length * objectives.mean_absolute_error(x, x_decoded_mean)\n",
    "            kl_loss = - 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis = -1)\n",
    "            return xent_loss + kl_loss\n",
    "\n",
    "        return (vae_loss, Lambda(sampling, output_shape=(latent_rep_size,), name='lambda')([z_mean, z_log_var]))\n",
    "\n",
    "    def _buildDecoder(self, z, latent_rep_size, max_length, channel_length):\n",
    "        h = Dense(latent_rep_size, name='latent_input', activation = 'relu')(z)\n",
    "        h = RepeatVector(max_length, name='repeat_vector')(h)\n",
    "        h = Dense(10, name='gru_1')(h)\n",
    "#        h = GRU(10, return_sequences = True, name='gru_1')(h)\n",
    "        return TimeDistributed(Dense(channel_length, activation='linear'), name='decoded_mean')(h)\n",
    "\n",
    "    def save(self, filename):\n",
    "        self.autoencoder.save_weights(filename)\n",
    "    \n",
    "    def load(self, charset, weights_file, latent_rep_size = 292):\n",
    "        self.create(charset, weights_file = weights_file, latent_rep_size = latent_rep_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# produce 3D vectors for training with CNN\n",
    "\n",
    "train_3D_ = train_.reshape(train_.shape[0], train_.shape[-1],1)\n",
    "train_vali_3D = train_vali.reshape(train_vali.shape[0], train_vali.shape[-1],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.2.2'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a baseline model to compare the performance between a vae encoded input and a raw input (666)\n",
    "train_encoded, test_encoded = encoding(vcnn_model.encoder, train=train_3D_, test= train_vali_3D)\n",
    "\n",
    "regressor = baseline_model(dim=[train_encoded.shape[1],200,1])\n",
    "history_regress = regressor.fit(train_encoded, train_label,\n",
    "        shuffle=True,\n",
    "        nb_epoch=50,\n",
    "        batch_size=100,\n",
    "        verbose = 1\n",
    "        )\n",
    "regressor.evaluate(test_encoded, vali_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_dim = np.shape(train_)[1]\n",
    "\n",
    "encoder, history = cnn_133(train_3D_, train_vali_3D, epoch_no = 20, \\\n",
    "                  encoder_dim=[input_dim, 666, 100], decoder_dim=[input_dim, 666, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def baseline_vcnn(dim= [1,2,3]):\n",
    "    \n",
    "    #define a baseline model only consisting of one hidden layer\n",
    "    \n",
    "    latent_rep_size = 2\n",
    "    batch_size = 100\n",
    "    \n",
    "    ##################\n",
    "\n",
    "    x = Input(shape=(dim[-2], dim[-1]))\n",
    "    h = Convolution1D(10, 11, activation = 'relu', name='conv_1')(x)\n",
    "    h = Flatten(name='flatten_1')(h)\n",
    "    h = Dense(10, activation = 'linear', name='dense_1')(h)\n",
    "    \n",
    "    z_mean = Dense(latent_rep_size, name='z_mean', activation = 'linear')(h)\n",
    "    z_log_var = Dense(latent_rep_size, name='z_log_var', activation = 'linear')(h)\n",
    "    \n",
    "    def sampling(args):\n",
    "        z_mean_, z_log_var_ = args\n",
    "        batch_size = K.shape(z_mean_)[0]\n",
    "        epsilon = K.random_normal(shape=(batch_size, latent_rep_size), mean=0.)\n",
    "        return z_mean_ + K.exp(z_log_var_ / 2) * epsilon\n",
    "\n",
    "    z = Lambda(sampling, output_shape=(latent_rep_size,))([z_mean, z_log_var])\n",
    "    \n",
    "    h_d = Dense(latent_rep_size, name='latent_input', activation = 'relu')(z)\n",
    "    h_d = RepeatVector(666, name='repeat_vector')(h_d)\n",
    "#    h_d = GRU(10, return_sequences = True, name='gru_3')(h_d)\n",
    "    \n",
    "    h_d = TimeDistributed(Dense(dim[-1], activation='linear'), name='decoded_mean')(h_d)\n",
    "\n",
    "    def vae_loss(x, x_decoded_mean):\n",
    "        x = K.flatten(x)\n",
    "        x_decoded_mean = K.flatten(x_decoded_mean)\n",
    "        xent_loss = max_length * objectives.binary_crossentropy(x, x_decoded_mean)\n",
    "        kl_loss = - 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis = -1)\n",
    "        return xent_loss + kl_loss\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    cnn = Model(x, h_d)\n",
    "\n",
    "    cnn.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "    return cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_dim = np.shape(train_)[1]\n",
    "\n",
    "vcnn_base = baseline_vcnn(dim=[input_dim, 666, 1])\n",
    "vcnn_base.fit(train_3D_, train_label,\n",
    "        shuffle=True,\n",
    "        nb_epoch=50,\n",
    "        batch_size=100,\n",
    "        verbose = 1,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def baseline_cnn(dim= [1,2,3]):\n",
    "    \n",
    "    #define a baseline model only consisting of one hidden layer\n",
    "\n",
    "    x = Input(shape=(dim[-2], dim[-1]))\n",
    "    h = Convolution1D(10, 11, activation = 'relu', name='conv_1')(x)\n",
    "    h = Flatten(name='flatten_1')(h)\n",
    "    h = Dense(1, activation = 'linear', name='dense_1')(h)\n",
    "    cnn = Model(x, h)\n",
    "\n",
    "    cnn.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "    return cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_dim = np.shape(train_)[1]\n",
    "\n",
    "cnn_base = baseline_cnn(dim=[input_dim, 666, 1])\n",
    "cnn_base.fit(train_3D_, train_label,\n",
    "        shuffle=True,\n",
    "        nb_epoch=50,\n",
    "        batch_size=100,\n",
    "        verbose = 1,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_dim = np.shape(train_)[1]\n",
    "\n",
    "encoder, history = vae_133(train_, train_vali, epoch_no = 20, \\\n",
    "                  encoder_dim=[input_dim, 600, 200], decoder_dim=[input_dim, 600, 200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build a baseline model to compare the performance between a vae encoded input and a raw input (666)\n",
    "train_encoded, test_encoded = encoding(encoder, train=train, test= test)\n",
    "\n",
    "regressor = baseline_model(dim=[train_encoded.shape[1],200,1])\n",
    "history_regress = regressor.fit(train_encoded, trainlabel_12,\n",
    "        shuffle=True,\n",
    "        nb_epoch=50,\n",
    "        batch_size=100,\n",
    "        verbose = 0,\n",
    "        )\n",
    "regressor.evaluate(test_encoded, testlabel_12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def baseline_model(dim= [1,2,3]):\n",
    "    \n",
    "    #define a baseline model only consisting of one hidden layer\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(dim[1], input_dim=dim[0],  activation='relu'))\n",
    "\tmodel.add(Dense(dim[-1], activation='linear'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load molecule data, the coordinate, atom_number, elements are essential as the input \n",
    "\n",
    "attrs_list =[ 'atom_number', 'data_base', 'atom_list', 'atom_coordinate_list', 'frequency_list','target_list',\\\n",
    "          'smile_list', 'atom_charge']\n",
    "\n",
    "metrics = [r2_score, mean_absolute_error, mean_squared_error, median_absolute_error]\n",
    "\n",
    "def load_133k(num = None):\n",
    "    \n",
    "    data_path = '/home/peng/Documents/Project_C/QSAR_nlp/Dataset_qm9/'\n",
    "    data_set = \"133K.hdf5\"\n",
    "\n",
    "    coord_list = []\n",
    "    element_list = []\n",
    "    target_list = []\n",
    "    atom_no_list = []\n",
    "    \n",
    "\n",
    "    f = h5py.File(data_path + data_set, \"r\")\n",
    "    if not num:\n",
    "        limit = len(f)+1\n",
    "    else:\n",
    "        limit = num\n",
    "        \n",
    "    for i in range(1, limit):\n",
    "        str_element = str(f[str(i)].attrs[attrs_list[2]], 'utf-8')\n",
    "        split_str = []\n",
    "        for j in str_element:\n",
    "            split_str.append(j) \n",
    "        \n",
    "\n",
    "        element_list.append(np.array(split_str))\n",
    "#        element_list.append(f[str(i)].attrs[attrs_list[2]])\n",
    "        coord_list.append(f[str(i)].attrs[attrs_list[3]])    \n",
    "        target_list.append(f[str(i)].attrs[attrs_list[-3]]) \n",
    "        atom_no_list.append(f[str(i)].attrs[attrs_list[0]])   \n",
    "        if i %100000 == 0:\n",
    "            print (\"already finish \", i)\n",
    "    f.close()\n",
    "    \n",
    "    #df = pd.DataFrame({'coord':coord_list, 'element':element_list, 'target':target_list})\n",
    "    \n",
    "    return element_list, coord_list, target_list, atom_no_list\n",
    "\n",
    "def get_attributes(element_list, coord_list, target_list):\n",
    "\n",
    "    fit_list = []\n",
    "#    target_single_list = []\n",
    "    for i in range(0, len(element_list)):\n",
    "        fit_list.append((element_list[i], coord_list[i]))\n",
    "#        target_single_list.append(target_list[i][-1])\n",
    "        \n",
    "    return fit_list #, target_single_list\n",
    "\n",
    "def get_train_list(num=None):\n",
    "\n",
    "    # extract the element, the coordinates, the targets and the atom_no from the 133k dataset\n",
    "    element_list, coord_list, target_list, atom_no_list = load_133k(num=num)\n",
    "    # generate the (element, coordinate) tuple for following feature engineering\n",
    "    fit_list = get_attributes(element_list, coord_list, target_list)\n",
    "\n",
    "    # generate feature engineered input features, CM or BOB\n",
    "    feature_methods = [CoulombMatrix(), BagOfBonds()]\n",
    "    feat_co = feature_methods[1]\n",
    "    train_list = feat_co.fit_transform(fit_list)\n",
    "    return train_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already finish  100000\n",
      "(133885, 666)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('bob_targets15.csv', header = 0)\n",
    "\n",
    "train_list = get_train_list()\n",
    "\n",
    "print (np.shape(train_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainlabel_12, testlabel_12 = train_test_split(df['12'], test_size = 0.2, random_state = 32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply minmax to train_list \n",
    "\n",
    "train_list_scale = MinMaxScaler().fit_transform(train_list)\n",
    "\n",
    "target_single_list = df['7']\n",
    "\n",
    "train, test, trainlabel, testlabel = train_test_split(train_list_scale, target_single_list,\\\n",
    "                                                      test_size=0.2, random_state = 32)\n",
    "\n",
    "train_, train_vali, train_label, vali_label = train_test_split(train, trainlabel, \\\n",
    "                                                              test_size=0.2, random_state = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('targets_15.csv', header = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[['5','6','7','9', '10','11','12','13']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.shape(train_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(history.history.keys())\n",
    "#plt.plot(history.history['loss'])\n",
    "#plt.plot(history.history['val_loss'])\n",
    "plt.plot(history.history['mean_absolute_error'])\n",
    "plt.plot(history.history['val_mean_absolute_error'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# try to use DAE \n",
    "input_dim = np.shape(train_)[1]\n",
    "\n",
    "encoder_dae, history_dae = dae_133(train_, train_vali, epoch_no = 20, \\\n",
    "                  encoder_dim=[input_dim, 600, 100], decoder_dim=[input_dim, 600, 100])\n",
    "\n",
    "train_encoded_dae, test_encoded_dae = encoding(encoder_dae, train=train, test= test)\n",
    "\n",
    "# build a baseline model to compare the performance between a vae encoded input and a raw input (666)\n",
    "regressor_dae = baseline_model(dim=[100,200,1])\n",
    "\n",
    "history_regress = regressor_dae.fit(train_encoded_dae, trainlabel,\n",
    "        shuffle=True,\n",
    "        nb_epoch=50,\n",
    "        batch_size=100,\n",
    "        verbose = 0,\n",
    "        )\n",
    "regressor_dae.evaluate(test_encoded_dae, testlabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build a model to project inputs on the latent space\n",
    "def encoding(encoder= None, train = train, test = test):\n",
    "    train_encoded = encoder.predict(train)\n",
    "    #print(np.shape(train)[1])\n",
    "    test_encoded = encoder.predict(test)\n",
    "    #print(np.shape(test))\n",
    "#    train_encoded_ = encoder.predict(train_)\n",
    "    #print(np.shape(train)[1])\n",
    "#    vali_encoded = encoder.predict(train_vali)\n",
    "    encoded_scale= MinMaxScaler().fit(train_encoded)\n",
    "    train_encoded_scale= encoded_scale.transform(train_encoded)\n",
    "    test_encoded_scale= encoded_scale.transform(test_encoded)\n",
    "    print 'the dimension for lattent representation is ', np.shape(train_encoded)[1]\n",
    "    return train_encoded_scale, test_encoded_scale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "color_label_int = [str(round(item/np.max(trainlabel))) for item in trainlabel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca_fit = PCA(n_components=100).fit(train)\n",
    "\n",
    "print (pca_fit.explained_variance_ratio_ )\n",
    "print (np.sum(pca_fit.explained_variance_ratio_))\n",
    "#print (np.shape(newtrain))\n",
    "\n",
    "pca_train = pca_fit.transform(train)\n",
    "pca_test = pca_fit.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dae_133(train, train_vali, epoch_no = 5, encoder_dim=[100, 100, 100], decoder_dim=[100, 100, 100]):\n",
    "    \n",
    "    ### Denosing autoencoder\n",
    "\n",
    "    noise_factor = 0.5\n",
    "    \n",
    "    train_noisy = train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=train.shape) \n",
    "\n",
    "\n",
    "    train_noisy = np.clip(train_noisy, 0., 1.)\n",
    "\n",
    "\n",
    "    # this is the size of our encoded representations\n",
    "    encoding_dim = 6  # 32 floats -> compression of factor 24.5, assuming the input is 784 floats\n",
    "\n",
    "    # this is our input placeholder\n",
    "    input_dim= Input(shape=(encoder_dim[0],))\n",
    "    # \"encoded\" is the encoded representation of the input\n",
    "    encoded = Dense(encoder_dim[1], activation='relu')(input_dim)\n",
    "    encoded = Dense(encoder_dim[-1], activation='relu', activity_regularizer=regularizers.activity_l1(10e-5))(encoded)\n",
    "\n",
    "    decoded = Dense(decoder_dim[1], activation='relu')(encoded)\n",
    "    decoded = Dense(decoder_dim[0], activation='relu')(decoded)\n",
    "\n",
    "\n",
    "    # this model maps an input to its reconstruction\n",
    "    autoencoder = Model(input=input_dim, output=decoded)\n",
    "\n",
    "    \n",
    "    autoencoder.compile(optimizer='sgd', loss='mae')\n",
    "\n",
    "    start = timeit.default_timer()\n",
    "    history = autoencoder.fit(train, train,\n",
    "                nb_epoch=epoch_no,\n",
    "                batch_size=100,\n",
    "                shuffle=True\n",
    "                )\n",
    "    encoder = Model(input=input_dim, output=encoded)\n",
    "\n",
    "    stop = timeit.default_timer()\n",
    "\n",
    "    print (\"The running takes %r min\" %((stop-start)/60))\n",
    "    \n",
    "    return encoder, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "estimator = rfr().fit(train_encoded, trainlabel)\n",
    "predict_test = estimator.predict(test_encoded)\n",
    "for i in metrics:\n",
    "    print i(testlabel, predict_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "estimator = rfr().fit(pca_train, trainlabel)\n",
    "predict_test = estimator.predict(pca_test)\n",
    "for i in metrics:\n",
    "    print i(testlabel, predict_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_all = []\n",
    "for i in range(0, len(element_list)):\n",
    "    for j in element_list[i]:\n",
    "        if j not in set(list_all):\n",
    "            list_all.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_atom = pd.DataFrame({'atom_no':atom_no_list})\n",
    "df_atom.describe()\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.style.use('ggplot')\n",
    "plt.figure()\n",
    "df_atom.plot.hist(alpha = 0.5, bins = 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "color_label = [str(item/np.max(train_label)) for item in train_label]\n",
    "plt.figure(figsize=(15,12))\n",
    "plt.scatter(train_encoded[:,0], train_encoded[:,1],marker = 'o', linewidth = '0', cmap = plt.cm.coolwarm,\\\n",
    "            c = color_label, s = 20, alpha = 1.0)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,12))\n",
    "plt.scatter(pca_train[:,0], pca_train[:,1] ,marker = 'o', linewidth = '0', cmap = plt.cm.coolwarm,\\\n",
    "            c = color_label, s = 40)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "generate targets, bob and store them into bob_targets15.csv\n",
    "\n",
    "# extract the element, the coordinates, the targets and the atom_no from the 133k dataset\n",
    "element_list, coord_list, target_list, atom_no_list = load_133k()\n",
    "# generate the (element, coordinate) tuple for following feature engineering\n",
    "fit_list = get_attributes(element_list, coord_list, target_list)\n",
    "\n",
    "# generate feature engineered input features, CM or BOB\n",
    "feature_methods = [CoulombMatrix(), BagOfBonds()]\n",
    "feat_co = feature_methods[1]\n",
    "train_list = feat_co.fit_transform(fit_list)\n",
    "\n",
    "df = pd.read_csv('targets_15.csv', header = 0)\n",
    "df['bob'] = train_list.tolist()\n",
    "df.to_csv('bob_targets15.csv', header = True)\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
