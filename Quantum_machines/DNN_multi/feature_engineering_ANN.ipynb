{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# in this script, different targets are predicted for comparing feature engineering techniques PCA, Kernel PCA,SDAE\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import molml \n",
    "from molml.features import CoulombMatrix, BagOfBonds\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import timeit\n",
    "from sklearn.metrics import r2_score,mean_squared_error, mean_absolute_error, median_absolute_error\n",
    "from sklearn.decomposition import PCA, KernelPCA, TruncatedSVD\n",
    "import seaborn as sns\n",
    "from IPython.core.pylabtools import figsize\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "# sys.path.append(\"/home/peng/git/molml/\")\n",
    "\n",
    "# from molml_local.features import CoulombMatrix\n",
    "# from molml_local.features import LocalCoulombMatrix\n",
    "# from molml_local.kernel import AtomKernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GTX 980 Ti (CNMeM is enabled with initial size: 75.0% of memory, cuDNN 5005)\n"
     ]
    }
   ],
   "source": [
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform, conditional\n",
    "from hyperopt import Trials, STATUS_OK, tpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Lambda, Dropout\n",
    "from keras.models import Model, load_model\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_pca_features(nb_pca=16, feature_list=None):\n",
    "#choose n components   \n",
    "    pca_fit = PCA(n_components=nb_pca).fit(feature_list[0])\n",
    "    pca_train = pca_fit.transform(feature_list[0])\n",
    "    \n",
    "    if np.shape(feature_list)[-1] == 3:\n",
    "        pca_vali = pca_fit.transform(feature_list[1])\n",
    "        pca_test = pca_fit.transform(feature_list[2])\n",
    "    \n",
    "        return [pca_train, pca_vali, pca_test], pca_fit\n",
    "    \n",
    "    elif np.shape(feature_list)[-1] == 2:\n",
    "        pca_test = pca_fit.transform(feature_list[-1])   \n",
    "        return [pca_train, pca_test], pca_fit\n",
    "    \n",
    "def get_pca_scale(pca_set):\n",
    "#abs scale the pca components     \n",
    "    max_abs=MaxAbsScaler()\n",
    "    \n",
    "    if np.shape(pca_set)[-1] == 3 :\n",
    "        \n",
    "        pca_set_scale=[0,0,0]\n",
    "        pca_set_scale[0]= max_abs.fit_transform(pca_set[0])\n",
    "        pca_set_scale[1], pca_set_scale[2]= max_abs.transform(pca_set[1]), max_abs.transform(pca_set[2])\n",
    "\n",
    "        return pca_set_scale\n",
    "    \n",
    "    if np.shape(pca_set)[-1] == 2 :\n",
    "        pca_set_scale=[0,0]\n",
    "        pca_set_scale[0]= max_abs.fit_transform(pca_set[0])\n",
    "        pca_set_scale[-1] = max_abs.transform(pca_set[-1])\n",
    "\n",
    "        return pca_set_scale \n",
    "    \n",
    "def get_nb_pca(ratio_list, thres_pca):\n",
    "    \n",
    "    for i in xrange(1, len(ratio_list)):\n",
    "        \n",
    "        if sum(ratio_list[:i])>thres_pca:\n",
    "            \n",
    "            nb_pca = i\n",
    "            break\n",
    "            \n",
    "    return nb_pca      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_fit_targets(target_list = None):\n",
    "    columns_list = target_list[0].columns\n",
    "    train_targets = []\n",
    "    vali_targets = []\n",
    "    for i in columns_list:\n",
    "        train_targets.append(np.array(target_list[0][i]))\n",
    "        vali_targets.append(np.array(target_list[1][i]))\n",
    "        \n",
    "    return train_targets, vali_targets    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_attributes(element_list, coord_list):\n",
    "    # produce [element, coord] lists for applying coulomb matrix and bob convertion\n",
    "\n",
    "    fit_list = []\n",
    "\n",
    "    for i in range(0, len(element_list)):\n",
    "        fit_list.append((element_list[i], coord_list[i]))\n",
    "        \n",
    "    return fit_list \n",
    "\n",
    "\n",
    "def get_train_vali_test(train_list, target_list, test_szie=0.2, random_state = 32):\n",
    "    \n",
    "    train, test, trainlabel, testlabel = train_test_split(train_list, target_list,\\\n",
    "                                                      test_size=0.2, random_state = 32)\n",
    "\n",
    "    train_, train_vali, train_label, vali_label = train_test_split(train, trainlabel, \\\n",
    "                                                              test_size=0.2, random_state = 32)\n",
    "    \n",
    "    return [train_, train_vali, test], [train_label, vali_label, testlabel]\n",
    "\n",
    "\n",
    "def get_train_test(train_list, target_list, test_szie=0.2, random_state = 32):\n",
    "    \n",
    "    train, test, trainlabel, testlabel = train_test_split(train_list, target_list,\\\n",
    "                                                      test_size=0.2, random_state = 32)\n",
    "    \n",
    "    return [train,  test], [trainlabel, testlabel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef get_sdae_features(feature_list, encoding_dim = 92):\\n    \\n    noise_factor = 0.5\\n    \\n    train_noise = feature_list[0] + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=feature_list[0].shape) \\n    vali_noise = feature_list[1] + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=feature_list[1].shape) \\n\\n\\n    train_noisy = np.clip(train_noise, 0., 1.)\\n    vali_noisy = np.clip(vali_noise, 0., 1.)\\n\\n    # this is the size of our encoded representations\\n      # 32 floats -> compression of factor 24.5, assuming the input is 784 floats\\n\\n    # this is our input placeholder\\n    \\n    input_dim= Input(shape=(np.shape(feature_list[0])[-1],))\\n    \\n    # \"encoded\" is the encoded representation of the input\\n    encoded = Dense(600, activation=\\'tanh\\')(input_dim)\\n    \\n    encoded = Dense(encoding_dim, activation=\\'tanh\\', activity_regularizer=regularizers.activity_l1(10e-5))(encoded)\\n\\n    decoded = Dense(np.shape(feature_list[0])[-1], activation=\\'linear\\')(encoded)\\n\\n\\n    # this model maps an input to its reconstruction\\n                    \\n    autoencoder = Model(input=input_dim, output=decoded)\\n\\n    encoder = Model(input=input_dim, output=encoded)\\n                     \\n    autoencoder.compile(optimizer=\\'sgd\\', loss=\\'mse\\')\\n\\n    start = timeit.default_timer()\\n                     \\n    history = autoencoder.fit(train_noisy, feature_list[0],\\n                    nb_epoch=100,\\n                    batch_size=100,\\n                    shuffle=True,\\n                    verbose = 0,\\n                    validation_data=(vali_noisy, feature_list[1])\\n                    )\\n\\n    stop = timeit.default_timer()\\n\\n    print (\"The running takes %r min\" %((stop-start)/60))    \\n    \\n    sdae_train, sdae_test = encoder.predict(feature_list[0]), encoder.predict(feature_list[1])\\n                     \\n    return [sdae_train, sdae_test], autoencoder, encoder, history\\n'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def get_sdae_features(feature_list, encoding_dim = 92):\n",
    "    \n",
    "    noise_factor = 0.5\n",
    "    \n",
    "    train_noise = feature_list[0] + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=feature_list[0].shape) \n",
    "    vali_noise = feature_list[1] + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=feature_list[1].shape) \n",
    "\n",
    "\n",
    "    train_noisy = np.clip(train_noise, 0., 1.)\n",
    "    vali_noisy = np.clip(vali_noise, 0., 1.)\n",
    "\n",
    "    # this is the size of our encoded representations\n",
    "      # 32 floats -> compression of factor 24.5, assuming the input is 784 floats\n",
    "\n",
    "    # this is our input placeholder\n",
    "    \n",
    "    input_dim= Input(shape=(np.shape(feature_list[0])[-1],))\n",
    "    \n",
    "    # \"encoded\" is the encoded representation of the input\n",
    "    encoded = Dense(600, activation='tanh')(input_dim)\n",
    "    \n",
    "    encoded = Dense(encoding_dim, activation='tanh', activity_regularizer=regularizers.activity_l1(10e-5))(encoded)\n",
    "\n",
    "    decoded = Dense(np.shape(feature_list[0])[-1], activation='linear')(encoded)\n",
    "\n",
    "\n",
    "    # this model maps an input to its reconstruction\n",
    "                    \n",
    "    autoencoder = Model(input=input_dim, output=decoded)\n",
    "\n",
    "    encoder = Model(input=input_dim, output=encoded)\n",
    "                     \n",
    "    autoencoder.compile(optimizer='sgd', loss='mse')\n",
    "\n",
    "    start = timeit.default_timer()\n",
    "                     \n",
    "    history = autoencoder.fit(train_noisy, feature_list[0],\n",
    "                    nb_epoch=100,\n",
    "                    batch_size=100,\n",
    "                    shuffle=True,\n",
    "                    verbose = 0,\n",
    "                    validation_data=(vali_noisy, feature_list[1])\n",
    "                    )\n",
    "\n",
    "    stop = timeit.default_timer()\n",
    "\n",
    "    print (\"The running takes %r min\" %((stop-start)/60))    \n",
    "    \n",
    "    sdae_train, sdae_test = encoder.predict(feature_list[0]), encoder.predict(feature_list[1])\n",
    "                     \n",
    "    return [sdae_train, sdae_test], autoencoder, encoder, history\n",
    "'''    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def data():\n",
    "    \n",
    "    df_data = pd.read_pickle('df_bob_target.pkl')\n",
    "    feature_list = df_data['feature_list']\n",
    "    \n",
    "    return feature_list\n",
    "\n",
    "\n",
    "def hyperas_sdae_features(feature_list, encoding_dim = 92):\n",
    "    \n",
    "    noise_factor = 0.5\n",
    "    \n",
    "    train_noise = feature_list[0] + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=feature_list[0].shape) \n",
    "    vali_noise = feature_list[1] + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=feature_list[1].shape) \n",
    "\n",
    "\n",
    "    train_noisy = np.clip(train_noise, 0., 1.)\n",
    "    vali_noisy = np.clip(vali_noise, 0., 1.)\n",
    "\n",
    "    # this is the size of our encoded representations\n",
    "      # 32 floats -> compression of factor 24.5, assuming the input is 784 floats\n",
    "\n",
    "    # this is our input placeholder\n",
    "    \n",
    "    input_dim= Input(shape=(np.shape(feature_list[0])[-1],))\n",
    "    \n",
    "    # \"encoded\" is the encoded representation of the input\n",
    "    \n",
    "    encoded = Dense({{choice([500, 600, 700])}}, activation='tanh')(input_dim)\n",
    "    \n",
    "    encoded = Dense(92, activation='tanh', activity_regularizer=regularizers.activity_l1(10e-5))(encoded)\n",
    "\n",
    "    decoded = Dense(np.shape(feature_list[0])[-1], activation='linear')(encoded)\n",
    "\n",
    "\n",
    "    # this model maps an input to its reconstruction\n",
    "                    \n",
    "    autoencoder = Model(input=input_dim, output=decoded)\n",
    "\n",
    "    encoder = Model(input=input_dim, output=encoded)\n",
    "                     \n",
    "    autoencoder.compile(optimizer='sgd', loss='mse')\n",
    "\n",
    "    start = timeit.default_timer()\n",
    "                     \n",
    "    history = autoencoder.fit(train_noisy, feature_list[0],\n",
    "                    nb_epoch=10,\n",
    "                    batch_size=100,\n",
    "                    shuffle=True,\n",
    "                    verbose = 0,\n",
    "                    validation_data=(vali_noisy, feature_list[1])\n",
    "                    )\n",
    "\n",
    "    stop = timeit.default_timer()\n",
    "\n",
    "    print (\"The running takes %r min\" %((stop-start)/60))    \n",
    "    \n",
    "    sdae_train, sdae_test = encoder.predict(feature_list[0]), encoder.predict(feature_list[-1])\n",
    "    \n",
    "    score, mse = autoencoder.evaluate(vali_noisy, feature_list[1], verbose = 0)\n",
    "    \n",
    "    print ('test mse: ', mse)\n",
    "                     \n",
    "    return {'loss': mse, 'status':STATUS_OK, 'model':hyperas_sdae_features}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import molml\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from molml.features import CoulombMatrix, BagOfBonds\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import sys\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import timeit\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, median_absolute_error\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.decomposition import PCA, KernelPCA, TruncatedSVD\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import seaborn as sns\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from IPython.core.pylabtools import figsize\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.cross_validation import StratifiedKFold\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform, conditional\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Input, Dense, Lambda, Dropout\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Model, load_model\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import regularizers\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'Dense': hp.choice('Dense', [500, 600, 700]),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      " 1: \n",
      " 2: \n",
      " 3: df_data = pd.read_pickle('df_bob_target.pkl')\n",
      " 4: feature_list = df_data['feature_list']\n",
      " 5: \n",
      " 6: \n",
      " 7: \n",
      " 8: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3:     \n",
      "   4:     noise_factor = 0.5\n",
      "   5:     \n",
      "   6:     train_noise = feature_list[0] + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=feature_list[0].shape) \n",
      "   7:     vali_noise = feature_list[1] + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=feature_list[1].shape) \n",
      "   8: \n",
      "   9: \n",
      "  10:     train_noisy = np.clip(train_noise, 0., 1.)\n",
      "  11:     vali_noisy = np.clip(vali_noise, 0., 1.)\n",
      "  12: \n",
      "  13:     # this is the size of our encoded representations\n",
      "  14:       # 32 floats -> compression of factor 24.5, assuming the input is 784 floats\n",
      "  15: \n",
      "  16:     # this is our input placeholder\n",
      "  17:     \n",
      "  18:     input_dim= Input(shape=(np.shape(feature_list[0])[-1],))\n",
      "  19:     \n",
      "  20:     # \"encoded\" is the encoded representation of the input\n",
      "  21:     \n",
      "  22:     encoded = Dense(space['Dense'], activation='tanh')(input_dim)\n",
      "  23:     \n",
      "  24:     encoded = Dense(92, activation='tanh', activity_regularizer=regularizers.activity_l1(10e-5))(encoded)\n",
      "  25: \n",
      "  26:     decoded = Dense(np.shape(feature_list[0])[-1], activation='linear')(encoded)\n",
      "  27: \n",
      "  28: \n",
      "  29:     # this model maps an input to its reconstruction\n",
      "  30:                     \n",
      "  31:     autoencoder = Model(input=input_dim, output=decoded)\n",
      "  32: \n",
      "  33:     encoder = Model(input=input_dim, output=encoded)\n",
      "  34:                      \n",
      "  35:     autoencoder.compile(optimizer='sgd', loss='mse')\n",
      "  36: \n",
      "  37:     start = timeit.default_timer()\n",
      "  38:                      \n",
      "  39:     history = autoencoder.fit(train_noisy, feature_list[0],\n",
      "  40:                     nb_epoch=10,\n",
      "  41:                     batch_size=100,\n",
      "  42:                     shuffle=True,\n",
      "  43:                     verbose = 0,\n",
      "  44:                     validation_data=(vali_noisy, feature_list[1])\n",
      "  45:                     )\n",
      "  46: \n",
      "  47:     stop = timeit.default_timer()\n",
      "  48: \n",
      "  49:     print (\"The running takes %r min\" %((stop-start)/60))    \n",
      "  50:     \n",
      "  51:     sdae_train, sdae_test = encoder.predict(feature_list[0]), encoder.predict(feature_list[-1])\n",
      "  52:     \n",
      "  53:     score, mse = autoencoder.evaluate(vali_noisy, feature_list[1], verbose = 0)\n",
      "  54:     \n",
      "  55:     print ('test mse: ', mse)\n",
      "  56:                      \n",
      "  57:     return {'loss': mse, 'status':STATUS_OK, 'model':hyperas_sdae_features}\n",
      "  58: \n",
      "The running takes 0.20235695044199625 min\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "-1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-7e7f2cf6c194>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m                                           \u001b[0mmax_evals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m                                           \u001b[0mtrials\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrials\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m                                           \u001b[0mnotebook_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'feature_engineering_ANN'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m                                          )\n",
      "\u001b[1;32m/home/peng/anaconda2/lib/python2.7/site-packages/hyperas/optim.pyc\u001b[0m in \u001b[0;36mminimize\u001b[1;34m(model, data, algo, max_evals, trials, functions, rseed, notebook_name, verbose, eval_space, return_space)\u001b[0m\n\u001b[0;32m     65\u001b[0m                                      \u001b[0mfull_model_string\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m                                      \u001b[0mnotebook_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnotebook_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m                                      verbose=verbose)\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[0mbest_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/peng/anaconda2/lib/python2.7/site-packages/hyperas/optim.pyc\u001b[0m in \u001b[0;36mbase_minimizer\u001b[1;34m(model, data, functions, algo, max_evals, trials, rseed, full_model_string, notebook_name, verbose, stack)\u001b[0m\n\u001b[0;32m    131\u001b[0m              \u001b[0mtrials\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m              \u001b[0mrstate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m              return_argmin=True),\n\u001b[0m\u001b[0;32m    134\u001b[0m         \u001b[0mget_space\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m     )\n",
      "\u001b[1;32m/home/peng/anaconda2/lib/python2.7/site-packages/hyperopt/fmin.pyc\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin)\u001b[0m\n\u001b[0;32m    305\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 307\u001b[1;33m             \u001b[0mreturn_argmin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m         )\n\u001b[0;32m    309\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/peng/anaconda2/lib/python2.7/site-packages/hyperopt/base.pyc\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(self, fn, space, algo, max_evals, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin)\u001b[0m\n\u001b[0;32m    633\u001b[0m             \u001b[0mpass_expr_memo_ctrl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpass_expr_memo_ctrl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 635\u001b[1;33m             return_argmin=return_argmin)\n\u001b[0m\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/peng/anaconda2/lib/python2.7/site-packages/hyperopt/fmin.pyc\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin)\u001b[0m\n\u001b[0;32m    318\u001b[0m                     verbose=verbose)\n\u001b[0;32m    319\u001b[0m     \u001b[0mrval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 320\u001b[1;33m     \u001b[0mrval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    321\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtrials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/peng/anaconda2/lib/python2.7/site-packages/hyperopt/fmin.pyc\u001b[0m in \u001b[0;36mexhaust\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/peng/anaconda2/lib/python2.7/site-packages/hyperopt/fmin.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, N, block_until_done)\u001b[0m\n\u001b[0;32m    171\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m                 \u001b[1;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstopped\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/peng/anaconda2/lib/python2.7/site-packages/hyperopt/fmin.pyc\u001b[0m in \u001b[0;36mserial_evaluate\u001b[1;34m(self, N)\u001b[0m\n\u001b[0;32m     90\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m                     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'job exception: %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/peng/anaconda2/lib/python2.7/site-packages/hyperopt/base.pyc\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[0;32m    838\u001b[0m                 \u001b[0mmemo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmemo\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    839\u001b[0m                 print_node_on_error=self.rec_eval_print_node_on_error)\n\u001b[1;32m--> 840\u001b[1;33m             \u001b[0mrval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    841\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    842\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mtemp_model.py\u001b[0m in \u001b[0;36mkeras_fmin_fnct\u001b[1;34m(space)\u001b[0m\n",
      "\u001b[1;32m/home/peng/anaconda2/lib/python2.7/site-packages/pandas/core/series.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    601\u001b[0m         \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 603\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    604\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/peng/anaconda2/lib/python2.7/site-packages/pandas/indexes/base.pyc\u001b[0m in \u001b[0;36mget_value\u001b[1;34m(self, series, key)\u001b[0m\n\u001b[0;32m   2167\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2168\u001b[0m             return self._engine.get_value(s, k,\n\u001b[1;32m-> 2169\u001b[1;33m                                           tz=getattr(series.dtype, 'tz', None))\n\u001b[0m\u001b[0;32m   2170\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2171\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minferred_type\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'integer'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'boolean'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_value (pandas/index.c:3557)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_value (pandas/index.c:3240)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:4279)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/src/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas.hashtable.Int64HashTable.get_item (pandas/hashtable.c:8564)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/src/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas.hashtable.Int64HashTable.get_item (pandas/hashtable.c:8508)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: -1"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    best_run, best_model = optim.minimize(model=hyperas_sdae_features,\n",
    "                                          data=data,\n",
    "                                          algo=tpe.suggest,\n",
    "                                          max_evals=5,\n",
    "                                          trials=Trials(),\n",
    "                                          notebook_name='feature_engineering_ANN'\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The running takes 1.757269299030304 min\n",
      "Running Fold 1 / 10\n",
      "Running Fold 2 / 10\n",
      "Running Fold 3 / 10\n",
      "Running Fold 4 / 10\n",
      "Running Fold 5 / 10\n",
      "Running Fold 6 / 10\n",
      "Running Fold 7 / 10\n",
      "Running Fold 8 / 10\n",
      "Running Fold 9 / 10\n",
      "Running Fold 10 / 10\n",
      "The running takes 7.669215047359467 min\n"
     ]
    }
   ],
   "source": [
    "## RR use sdae to reconstruct the input and use 10-fold cv to get models\n",
    "'''\n",
    "sdae_set, autoencoder, encoder, history = get_sdae_features(feature_list=feature_list)\n",
    "\n",
    "sdae_set_scale = get_pca_scale(sdae_set)\n",
    "\n",
    "cv_model_list_sdae99, cv_abs_list_sdae99 = cross_vali_training(sdae_set_scale, target_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "dic_results_sda = {}\n",
    "dic_results_sda['sda_models_92']=cv_model_list_sdae99\n",
    "dic_results_sda['sda_abs_92']=cv_abs_list_sdae99\n",
    "df_sda = pd.DataFrame(dic_results_sda)\n",
    "df_sda.to_pickle('./Results/dic_sda.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#PCA1. Normalize the raw features (BOB and CM)\n",
    "\n",
    "#feature_list, target_list = get_train_vali_test(train_list_scale_abs, df['gap'])\n",
    "#feature_list, target_list = get_train_vali_test(train_list_scale_abs, df['gap'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#PCA2. get and scale the new PCA features 0.90\n",
    "'''\n",
    "\n",
    "pca_fit_ = PCA().fit(feature_list[0])\n",
    "\n",
    "nb_pca_ = get_nb_pca(pca_fit_.explained_variance_ratio_, 0.9)\n",
    "\n",
    "pca_set, pca_fit =get_pca_features(nb_pca=16, feature_list=feature_list)\n",
    "\n",
    "pca_set_scale = get_pca_scale(pca_set)\n",
    "\n",
    "cv_model_list, cv_abs_list = cross_vali_training(pca_set_scale, target_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#PCA2. get and scale the new PCA features 0.99\n",
    "'''\n",
    "\n",
    "pca_fit_ = PCA().fit(feature_list[0])\n",
    "\n",
    "ratio_list = pca_fit_.explained_variance_ratio_\n",
    "\n",
    "nb_pca_ = get_nb_pca(ratio_list, 0.99)\n",
    "\n",
    "pca_set, pca_fit =get_pca_features(nb_pca=nb_pca_, feature_list=feature_list)\n",
    "\n",
    "pca_set_scale = get_pca_scale(pca_set)\n",
    "\n",
    "cv_model_list_pca99, cv_abs_list_pca99 = cross_vali_training(pca_set_scale, target_list)\n",
    "\n",
    "#dic_cv_results['pca_abs_95']= cv_abs_list_pca99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#KPCA1. get and scale the new PCA features 0.99\n",
    "'''\n",
    "\n",
    "pca_fit_ = TruncatedSVD(n_components=100).fit(feature_list[1])\n",
    "\n",
    "ratio_list = pca_fit_.explained_variance_ratio_\n",
    "\n",
    "nb_pca_ = get_nb_pca(ratio_list, 0.9)\n",
    "\n",
    "pca_set, pca_fit =get_pca_features(nb_pca=nb_pca_, feature_list=feature_list)\n",
    "\n",
    "pca_set_scale = get_pca_scale(pca_set)\n",
    "\n",
    "cv_model_list_svd, cv_abs_list_svd = cross_vali_training(pca_set_scale, target_list)\n",
    "\n",
    "#dic_cv_results['pca_abs_95']= cv_abs_list_pca99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cv_training(model, feature_train, target_train, feature_test, target_test, epochs = 100):\n",
    "#PCA3. cross validation during the training  \n",
    "\n",
    "    model.fit(feature_train, target_train,\\\n",
    "                                    shuffle=True,\n",
    "                nb_epoch=epochs,\n",
    "                batch_size=100,\n",
    "                verbose = 0)\n",
    "    \n",
    "    metric_abs=model.evaluate(feature_test, target_test, verbose = 0) \n",
    "    \n",
    "    return model, metric_abs\n",
    "\n",
    "def cross_vali_training(training_set, target_set):\n",
    "#PCA3. cross validation during the training        \n",
    "\n",
    "    start = timeit.default_timer()\n",
    "\n",
    "    n_folds = 10\n",
    "\n",
    "    skf = StratifiedKFold(target_list[0], n_folds=n_folds, shuffle=True)\n",
    "\n",
    "    cv_model_list = []\n",
    "    cv_abs_list = []\n",
    "\n",
    "    for i, (train, test) in enumerate(skf):\n",
    "\n",
    "\n",
    "        print \"Running Fold\", i+1, \"/\", n_folds\n",
    "\n",
    "\n",
    "        model_cv = None # Clearing the NN.\n",
    "\n",
    "        model_cv = baseline_ann(np.shape(training_set[0])[-1])\n",
    "\n",
    "        model_fitted, abs_cv = cv_training(model_cv,\\\n",
    "                                np.array(training_set[0])[train], np.array(target_set[0])[train],\\\n",
    "                                np.array(training_set[0])[test], np.array(target_set[0])[test], epochs=100)\n",
    "        \n",
    "        cv_model_list.append(model_fitted)\n",
    "\n",
    "        cv_abs_list.append(abs_cv)\n",
    "        \n",
    "    stop = timeit.default_timer()\n",
    "\n",
    "    print (\"The running takes %r min\" %((stop-start)/60)) \n",
    "    \n",
    "    return cv_model_list, cv_abs_list    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "\n",
    "pca_fit_ = TruncatedSVD(n_components=100).fit(feature_list[1])\n",
    "\n",
    "ratio_list = pca_fit_.explained_variance_ratio_\n",
    "\n",
    "nb_pca_ = get_nb_pca(ratio_list, 0.99)\n",
    "\n",
    "pca_set, pca_fit =get_pca_features(nb_pca=nb_pca_, feature_list=feature_list)\n",
    "\n",
    "pca_set_scale = get_pca_scale(pca_set)\n",
    "\n",
    "cv_model_list_svd99, cv_abs_list_svd99 = cross_vali_training(pca_set_scale, target_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "print np.mean(dic_cv_results['base_abs']), np.mean(dic_cv_results['pca_abs']),\\\n",
    "np.mean(dic_cv_results['pca_abs_95']), np.mean(dic_cv_results['pca_abs_99'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "dic_cv_results['base_models'][2].evaluate(feature_list[-1], target_list[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "dic_cv_results['base_models']=cv_model_list_base\n",
    "dic_cv_results['base_abs']=cv_abs_list_base\n",
    "dic_cv_results['pca_models']= cv_model_list\n",
    "dic_cv_results['pca_abs']= cv_abs_list\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def baseline_ann(input_dim=666):\n",
    "    \n",
    "    x = Input(shape=(input_dim, ))\n",
    "    h1 = Dense(1000, activation = 'tanh')(x)\n",
    "    o1 = Dense(1, activation = 'linear', name='o1')(h1)\n",
    "    \n",
    "    model = Model(x, o1)\n",
    "    \n",
    "    model.compile(loss='mean_absolute_error', optimizer = 'adam')\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "# plot PCA rang\n",
    "import pylab as pb\n",
    "pb.axes(frameon = 0)\n",
    "\n",
    "figsize(10,8)\n",
    "plot_std = 0\n",
    "color ='gray'\n",
    "n=16\n",
    "#print(forest.feature_importances_)\n",
    "importances = pca_fit.explained_variance_ratio_[:n]\n",
    "#return importances\n",
    "#std=np.std([tree.feature_importances_ for tree in forest.estimators_],axis=0)\n",
    "indices=np.argsort(importances)[::-1]\n",
    "#print indices\n",
    "print(\"Feature ranking:\")\n",
    "for f in range(12):\n",
    "      print(\"%d. feature %d (%f)\" % (f + 1, indices[f]+1, importances[indices[f]]))\n",
    "#plt.figure(figsize=(8,6.5))\n",
    "\n",
    "\n",
    "if plot_std == True:\n",
    "    pass                                 \n",
    "    #plt.bar(range(12), importances[indices],\n",
    "    #color=color, yerr=std[indices], align=\"center\")\n",
    "\n",
    "else:\n",
    "    plt.bar(range(n), importances[indices],\n",
    "    color=color, align=\"center\")\n",
    "\n",
    "plt.xticks(range(n), indices+1, fontsize=20)\n",
    "plt.yticks(fontsize = 20)\n",
    "plt.xlim([-1, n])\n",
    "#        plt.ylim([0.00,0.30])\n",
    "plt.xlabel('Components', fontsize=24)\n",
    "plt.ylabel('Eigenvalues', fontsize=24)\n",
    "#ax.spines['right'].set_visible(False)\n",
    "#ax.spines['top'].set_visible(False)\n",
    "#plt.savefig(paper_path +\"pca_rank.tif\", dpi=600)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "print sum(pca_fit.explained_variance_ratio_[:100])\n",
    "#df_pca_train = pd.DataFrame(pca_train)\n",
    "#df_pca_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_single_dnn():\n",
    "\n",
    "    x = Input(shape=(666, ))\n",
    "    \n",
    "    h1 = Dense(1000, activation = 'relu')(x)\n",
    "#    h1 = Dropout(0.1)(h1)\n",
    "    h1 = Dense(1000, activation = 'relu')(h1)\n",
    "#    h1 = Dropout(0.1)(h1)\n",
    "    h1 = Dense(1000, activation = 'relu')(h1)\n",
    "    \n",
    "    o1 = Dense(1, activation = 'linear', name='o1')(h1)\n",
    "#     o2 = Dense(1, activation = 'linear', name='o2')(h1)\n",
    "#     o3 = Dense(1, activation = 'linear', name='o3')(h1)\n",
    "#     o5 = Dense(1, activation = 'linear', name='o4')(h1)\n",
    "#     o5 = Dense(1, activation = 'linear', name='o5')(h1)\n",
    "#     o6 = Dense(1, activation = 'linear', name='o6')(h1)\n",
    "    \n",
    "    model = Model(x, o1)\n",
    "    \n",
    "  \n",
    "    model.compile(loss='mean_absolute_error', optimizer = 'adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 0. Some setting\n",
    "\n",
    "\n",
    "\n",
    "other_list = ['A', 'B', 'C']\n",
    "\n",
    "elec_list = ['dipole', 'polar','spatialSize', 'homo', 'lumo', 'gap']\n",
    "\n",
    "thermo_list = ['U0', 'U', 'H', 'G', 'Cv','zpe']\n",
    "\n",
    "# metrics = [r2_score, mean_absolute_error, mean_squared_error, median_absolute_error]\n",
    "\n",
    "# results_list = []\n",
    "\n",
    "# dic_cv_results = {}\n",
    "\n",
    "# data_path_pkl = '/home/peng/Documents/Project_C/QSAR_nlp/Dataset_qm9/'\n",
    "\n",
    "# df = pd.read_pickle(data_path_pkl + 'filter_133k.pkl')\n",
    "\n",
    "# fit_list = get_attributes(np.array(df['atomList']), np.array(df['atomCoords']))\n",
    "\n",
    "# feature_methods = [CoulombMatrix(), BagOfBonds()]\n",
    "\n",
    "# feat_co = feature_methods[1]\n",
    "\n",
    "# train_list = feat_co.fit_transform(fit_list)\n",
    "\n",
    "# train_list_scale = MinMaxScaler().fit_transform(train_list)\n",
    "\n",
    "# train_list_scale_abs = MaxAbsScaler().fit_transform(train_list)\n",
    "\n",
    "# feature_list, target_list = get_train_vali_test(train_list_scale_abs, df['gap'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# 1. data importing\n",
    "\n",
    "\n",
    "data_path_pkl = '/home/peng/Documents/Project_C/QSAR_nlp/Dataset_qm9/'\n",
    "\n",
    "df = pd.read_pickle(data_path_pkl + 'filter_133k.pkl')\n",
    "\n",
    "#print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spatialSize',\n",
       " 'polar',\n",
       " 'C',\n",
       " 'B',\n",
       " 'A',\n",
       " 'G',\n",
       " 'H',\n",
       " 'dipole',\n",
       " 'gap',\n",
       " 'U0',\n",
       " 'lumo',\n",
       " 'homo',\n",
       " 'U',\n",
       " 'zpe',\n",
       " 'Cv']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_all = list(set(elec_list)|set(thermo_list)|set(other_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spatialSize</th>\n",
       "      <th>polar</th>\n",
       "      <th>C</th>\n",
       "      <th>B</th>\n",
       "      <th>A</th>\n",
       "      <th>G</th>\n",
       "      <th>H</th>\n",
       "      <th>dipole</th>\n",
       "      <th>gap</th>\n",
       "      <th>U0</th>\n",
       "      <th>lumo</th>\n",
       "      <th>homo</th>\n",
       "      <th>U</th>\n",
       "      <th>zpe</th>\n",
       "      <th>Cv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>35.3641</td>\n",
       "      <td>13.21</td>\n",
       "      <td>157.706990</td>\n",
       "      <td>157.709970</td>\n",
       "      <td>157.71180</td>\n",
       "      <td>-40.498597</td>\n",
       "      <td>-40.475117</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5048</td>\n",
       "      <td>-40.478930</td>\n",
       "      <td>0.1171</td>\n",
       "      <td>-0.3877</td>\n",
       "      <td>-40.476062</td>\n",
       "      <td>0.044749</td>\n",
       "      <td>6.469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26.1563</td>\n",
       "      <td>9.46</td>\n",
       "      <td>191.393970</td>\n",
       "      <td>293.541110</td>\n",
       "      <td>293.60975</td>\n",
       "      <td>-56.544961</td>\n",
       "      <td>-56.522082</td>\n",
       "      <td>1.6256</td>\n",
       "      <td>0.3399</td>\n",
       "      <td>-56.525887</td>\n",
       "      <td>0.0829</td>\n",
       "      <td>-0.2570</td>\n",
       "      <td>-56.523026</td>\n",
       "      <td>0.034358</td>\n",
       "      <td>6.316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.0002</td>\n",
       "      <td>6.31</td>\n",
       "      <td>282.945450</td>\n",
       "      <td>437.903860</td>\n",
       "      <td>799.58812</td>\n",
       "      <td>-76.422349</td>\n",
       "      <td>-76.400922</td>\n",
       "      <td>1.8511</td>\n",
       "      <td>0.3615</td>\n",
       "      <td>-76.404702</td>\n",
       "      <td>0.0687</td>\n",
       "      <td>-0.2928</td>\n",
       "      <td>-76.401867</td>\n",
       "      <td>0.021375</td>\n",
       "      <td>6.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59.5248</td>\n",
       "      <td>16.28</td>\n",
       "      <td>35.610036</td>\n",
       "      <td>35.610036</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-77.327429</td>\n",
       "      <td>-77.304583</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3351</td>\n",
       "      <td>-77.308427</td>\n",
       "      <td>0.0506</td>\n",
       "      <td>-0.2845</td>\n",
       "      <td>-77.305527</td>\n",
       "      <td>0.026841</td>\n",
       "      <td>8.574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>48.7476</td>\n",
       "      <td>12.99</td>\n",
       "      <td>44.593883</td>\n",
       "      <td>44.593883</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-93.431246</td>\n",
       "      <td>-93.408425</td>\n",
       "      <td>2.8937</td>\n",
       "      <td>0.3796</td>\n",
       "      <td>-93.411888</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>-0.3604</td>\n",
       "      <td>-93.409370</td>\n",
       "      <td>0.016601</td>\n",
       "      <td>6.278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>59.9891</td>\n",
       "      <td>14.18</td>\n",
       "      <td>34.298920</td>\n",
       "      <td>38.982300</td>\n",
       "      <td>285.48839</td>\n",
       "      <td>-114.505268</td>\n",
       "      <td>-114.479802</td>\n",
       "      <td>2.1089</td>\n",
       "      <td>0.2263</td>\n",
       "      <td>-114.483613</td>\n",
       "      <td>-0.0406</td>\n",
       "      <td>-0.2670</td>\n",
       "      <td>-114.480746</td>\n",
       "      <td>0.026603</td>\n",
       "      <td>6.413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>109.5031</td>\n",
       "      <td>23.95</td>\n",
       "      <td>19.906330</td>\n",
       "      <td>19.906490</td>\n",
       "      <td>80.46225</td>\n",
       "      <td>-79.787269</td>\n",
       "      <td>-79.759722</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4426</td>\n",
       "      <td>-79.764152</td>\n",
       "      <td>0.1041</td>\n",
       "      <td>-0.3385</td>\n",
       "      <td>-79.760666</td>\n",
       "      <td>0.074542</td>\n",
       "      <td>10.098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>83.7940</td>\n",
       "      <td>16.97</td>\n",
       "      <td>23.978720</td>\n",
       "      <td>24.858720</td>\n",
       "      <td>127.83497</td>\n",
       "      <td>-115.701876</td>\n",
       "      <td>-115.674872</td>\n",
       "      <td>1.5258</td>\n",
       "      <td>0.3437</td>\n",
       "      <td>-115.679136</td>\n",
       "      <td>0.0784</td>\n",
       "      <td>-0.2653</td>\n",
       "      <td>-115.675816</td>\n",
       "      <td>0.051208</td>\n",
       "      <td>8.751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>177.1963</td>\n",
       "      <td>28.78</td>\n",
       "      <td>8.593210</td>\n",
       "      <td>8.593230</td>\n",
       "      <td>160.28041</td>\n",
       "      <td>-116.633775</td>\n",
       "      <td>-116.604606</td>\n",
       "      <td>0.7156</td>\n",
       "      <td>0.3222</td>\n",
       "      <td>-116.609549</td>\n",
       "      <td>0.0613</td>\n",
       "      <td>-0.2609</td>\n",
       "      <td>-116.605550</td>\n",
       "      <td>0.055410</td>\n",
       "      <td>12.482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>160.7223</td>\n",
       "      <td>24.45</td>\n",
       "      <td>9.223240</td>\n",
       "      <td>9.223270</td>\n",
       "      <td>159.03567</td>\n",
       "      <td>-132.742149</td>\n",
       "      <td>-132.713619</td>\n",
       "      <td>3.8266</td>\n",
       "      <td>0.3640</td>\n",
       "      <td>-132.718150</td>\n",
       "      <td>0.0376</td>\n",
       "      <td>-0.3264</td>\n",
       "      <td>-132.714563</td>\n",
       "      <td>0.045286</td>\n",
       "      <td>10.287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>166.9728</td>\n",
       "      <td>25.11</td>\n",
       "      <td>9.073680</td>\n",
       "      <td>10.111220</td>\n",
       "      <td>57.22434</td>\n",
       "      <td>-153.812518</td>\n",
       "      <td>-153.782784</td>\n",
       "      <td>2.5682</td>\n",
       "      <td>0.2342</td>\n",
       "      <td>-153.787612</td>\n",
       "      <td>-0.0198</td>\n",
       "      <td>-0.2540</td>\n",
       "      <td>-153.783728</td>\n",
       "      <td>0.055355</td>\n",
       "      <td>11.219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>145.3078</td>\n",
       "      <td>21.57</td>\n",
       "      <td>9.836390</td>\n",
       "      <td>11.347930</td>\n",
       "      <td>73.84720</td>\n",
       "      <td>-169.885594</td>\n",
       "      <td>-169.855958</td>\n",
       "      <td>3.7286</td>\n",
       "      <td>0.2845</td>\n",
       "      <td>-169.860788</td>\n",
       "      <td>0.0302</td>\n",
       "      <td>-0.2543</td>\n",
       "      <td>-169.856903</td>\n",
       "      <td>0.045279</td>\n",
       "      <td>10.890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>227.1361</td>\n",
       "      <td>34.75</td>\n",
       "      <td>7.420760</td>\n",
       "      <td>8.377010</td>\n",
       "      <td>29.45018</td>\n",
       "      <td>-119.078157</td>\n",
       "      <td>-119.046983</td>\n",
       "      <td>0.0597</td>\n",
       "      <td>0.4179</td>\n",
       "      <td>-119.052475</td>\n",
       "      <td>0.0949</td>\n",
       "      <td>-0.3230</td>\n",
       "      <td>-119.047927</td>\n",
       "      <td>0.103182</td>\n",
       "      <td>14.840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>193.1659</td>\n",
       "      <td>27.87</td>\n",
       "      <td>8.149700</td>\n",
       "      <td>9.368600</td>\n",
       "      <td>35.09545</td>\n",
       "      <td>-154.998148</td>\n",
       "      <td>-154.967467</td>\n",
       "      <td>1.4131</td>\n",
       "      <td>0.3417</td>\n",
       "      <td>-154.972731</td>\n",
       "      <td>0.0798</td>\n",
       "      <td>-0.2619</td>\n",
       "      <td>-154.968412</td>\n",
       "      <td>0.079754</td>\n",
       "      <td>13.546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>187.1015</td>\n",
       "      <td>28.13</td>\n",
       "      <td>8.903530</td>\n",
       "      <td>10.040330</td>\n",
       "      <td>39.37691</td>\n",
       "      <td>-154.985747</td>\n",
       "      <td>-154.955100</td>\n",
       "      <td>1.1502</td>\n",
       "      <td>0.3435</td>\n",
       "      <td>-154.960361</td>\n",
       "      <td>0.0910</td>\n",
       "      <td>-0.2525</td>\n",
       "      <td>-154.956045</td>\n",
       "      <td>0.079534</td>\n",
       "      <td>12.934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>155.8145</td>\n",
       "      <td>30.82</td>\n",
       "      <td>12.589100</td>\n",
       "      <td>20.153020</td>\n",
       "      <td>20.15852</td>\n",
       "      <td>-117.849087</td>\n",
       "      <td>-117.820482</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.3930</td>\n",
       "      <td>-117.824798</td>\n",
       "      <td>0.1042</td>\n",
       "      <td>-0.2888</td>\n",
       "      <td>-117.821426</td>\n",
       "      <td>0.081231</td>\n",
       "      <td>11.041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>129.8910</td>\n",
       "      <td>24.04</td>\n",
       "      <td>14.180660</td>\n",
       "      <td>22.121090</td>\n",
       "      <td>25.77385</td>\n",
       "      <td>-153.766642</td>\n",
       "      <td>-153.738466</td>\n",
       "      <td>1.7675</td>\n",
       "      <td>0.3724</td>\n",
       "      <td>-153.742562</td>\n",
       "      <td>0.1042</td>\n",
       "      <td>-0.2682</td>\n",
       "      <td>-153.739410</td>\n",
       "      <td>0.057289</td>\n",
       "      <td>9.176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>292.4367</td>\n",
       "      <td>35.53</td>\n",
       "      <td>4.896150</td>\n",
       "      <td>8.490110</td>\n",
       "      <td>10.12193</td>\n",
       "      <td>-193.116476</td>\n",
       "      <td>-193.082024</td>\n",
       "      <td>2.7362</td>\n",
       "      <td>0.2344</td>\n",
       "      <td>-193.088340</td>\n",
       "      <td>-0.0087</td>\n",
       "      <td>-0.2431</td>\n",
       "      <td>-193.082969</td>\n",
       "      <td>0.083382</td>\n",
       "      <td>16.893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>267.6148</td>\n",
       "      <td>31.83</td>\n",
       "      <td>5.149770</td>\n",
       "      <td>9.275090</td>\n",
       "      <td>10.78886</td>\n",
       "      <td>-209.187468</td>\n",
       "      <td>-209.153076</td>\n",
       "      <td>3.6367</td>\n",
       "      <td>0.2783</td>\n",
       "      <td>-209.159302</td>\n",
       "      <td>0.0347</td>\n",
       "      <td>-0.2436</td>\n",
       "      <td>-209.154020</td>\n",
       "      <td>0.073190</td>\n",
       "      <td>16.561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>244.2308</td>\n",
       "      <td>28.07</td>\n",
       "      <td>5.420910</td>\n",
       "      <td>10.363880</td>\n",
       "      <td>11.14750</td>\n",
       "      <td>-225.247724</td>\n",
       "      <td>-225.216131</td>\n",
       "      <td>3.4869</td>\n",
       "      <td>0.3051</td>\n",
       "      <td>-225.221461</td>\n",
       "      <td>0.0556</td>\n",
       "      <td>-0.2495</td>\n",
       "      <td>-225.217075</td>\n",
       "      <td>0.063824</td>\n",
       "      <td>15.292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>355.0621</td>\n",
       "      <td>45.46</td>\n",
       "      <td>4.486680</td>\n",
       "      <td>7.748470</td>\n",
       "      <td>7.75166</td>\n",
       "      <td>-158.370016</td>\n",
       "      <td>-158.335658</td>\n",
       "      <td>0.0897</td>\n",
       "      <td>0.4010</td>\n",
       "      <td>-158.342346</td>\n",
       "      <td>0.0843</td>\n",
       "      <td>-0.3167</td>\n",
       "      <td>-158.336603</td>\n",
       "      <td>0.131146</td>\n",
       "      <td>20.273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>318.3721</td>\n",
       "      <td>38.58</td>\n",
       "      <td>4.755420</td>\n",
       "      <td>8.005680</td>\n",
       "      <td>8.67553</td>\n",
       "      <td>-194.294663</td>\n",
       "      <td>-194.260804</td>\n",
       "      <td>1.4259</td>\n",
       "      <td>0.3351</td>\n",
       "      <td>-194.267232</td>\n",
       "      <td>0.0740</td>\n",
       "      <td>-0.2612</td>\n",
       "      <td>-194.261748</td>\n",
       "      <td>0.107673</td>\n",
       "      <td>19.052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>278.6264</td>\n",
       "      <td>38.52</td>\n",
       "      <td>4.425973</td>\n",
       "      <td>4.425973</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-153.482621</td>\n",
       "      <td>-153.454498</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2386</td>\n",
       "      <td>-153.459846</td>\n",
       "      <td>-0.0214</td>\n",
       "      <td>-0.2599</td>\n",
       "      <td>-153.455442</td>\n",
       "      <td>0.037354</td>\n",
       "      <td>15.312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>260.1896</td>\n",
       "      <td>32.66</td>\n",
       "      <td>4.579322</td>\n",
       "      <td>4.579322</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-169.581024</td>\n",
       "      <td>-169.552820</td>\n",
       "      <td>3.7920</td>\n",
       "      <td>0.2559</td>\n",
       "      <td>-169.557758</td>\n",
       "      <td>-0.0543</td>\n",
       "      <td>-0.3102</td>\n",
       "      <td>-169.553764</td>\n",
       "      <td>0.027259</td>\n",
       "      <td>12.930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>242.9308</td>\n",
       "      <td>27.70</td>\n",
       "      <td>4.732690</td>\n",
       "      <td>4.732690</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-185.667652</td>\n",
       "      <td>-185.643880</td>\n",
       "      <td>0.0023</td>\n",
       "      <td>0.2770</td>\n",
       "      <td>-185.648533</td>\n",
       "      <td>-0.0926</td>\n",
       "      <td>-0.3696</td>\n",
       "      <td>-185.644825</td>\n",
       "      <td>0.015951</td>\n",
       "      <td>10.398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>268.3921</td>\n",
       "      <td>31.14</td>\n",
       "      <td>4.515860</td>\n",
       "      <td>4.834500</td>\n",
       "      <td>68.51710</td>\n",
       "      <td>-190.650543</td>\n",
       "      <td>-190.619419</td>\n",
       "      <td>2.7824</td>\n",
       "      <td>0.2042</td>\n",
       "      <td>-190.624631</td>\n",
       "      <td>-0.0735</td>\n",
       "      <td>-0.2777</td>\n",
       "      <td>-190.620363</td>\n",
       "      <td>0.037208</td>\n",
       "      <td>13.049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>251.0007</td>\n",
       "      <td>26.25</td>\n",
       "      <td>4.664130</td>\n",
       "      <td>5.008230</td>\n",
       "      <td>67.88408</td>\n",
       "      <td>-206.747625</td>\n",
       "      <td>-206.716931</td>\n",
       "      <td>2.3112</td>\n",
       "      <td>0.2066</td>\n",
       "      <td>-206.721858</td>\n",
       "      <td>-0.1100</td>\n",
       "      <td>-0.3166</td>\n",
       "      <td>-206.717875</td>\n",
       "      <td>0.026540</td>\n",
       "      <td>11.329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>266.8164</td>\n",
       "      <td>26.12</td>\n",
       "      <td>4.400880</td>\n",
       "      <td>4.774410</td>\n",
       "      <td>56.25048</td>\n",
       "      <td>-227.825074</td>\n",
       "      <td>-227.793626</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.1555</td>\n",
       "      <td>-227.798785</td>\n",
       "      <td>-0.1113</td>\n",
       "      <td>-0.2668</td>\n",
       "      <td>-227.794570</td>\n",
       "      <td>0.036943</td>\n",
       "      <td>12.147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>400.2236</td>\n",
       "      <td>42.32</td>\n",
       "      <td>3.368800</td>\n",
       "      <td>3.368800</td>\n",
       "      <td>80.28766</td>\n",
       "      <td>-155.937641</td>\n",
       "      <td>-155.902236</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3096</td>\n",
       "      <td>-155.908941</td>\n",
       "      <td>0.0684</td>\n",
       "      <td>-0.2412</td>\n",
       "      <td>-155.903180</td>\n",
       "      <td>0.083896</td>\n",
       "      <td>17.447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>333.9589</td>\n",
       "      <td>40.09</td>\n",
       "      <td>4.083420</td>\n",
       "      <td>4.530050</td>\n",
       "      <td>27.39459</td>\n",
       "      <td>-155.924226</td>\n",
       "      <td>-155.891347</td>\n",
       "      <td>0.7067</td>\n",
       "      <td>0.3157</td>\n",
       "      <td>-155.897345</td>\n",
       "      <td>0.0566</td>\n",
       "      <td>-0.2592</td>\n",
       "      <td>-155.892291</td>\n",
       "      <td>0.084338</td>\n",
       "      <td>17.130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133830</th>\n",
       "      <td>832.1811</td>\n",
       "      <td>80.36</td>\n",
       "      <td>1.899860</td>\n",
       "      <td>2.064040</td>\n",
       "      <td>3.14041</td>\n",
       "      <td>-364.709768</td>\n",
       "      <td>-364.671183</td>\n",
       "      <td>3.7345</td>\n",
       "      <td>0.1985</td>\n",
       "      <td>-364.679063</td>\n",
       "      <td>0.0134</td>\n",
       "      <td>-0.1852</td>\n",
       "      <td>-364.672127</td>\n",
       "      <td>0.147709</td>\n",
       "      <td>28.778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133834</th>\n",
       "      <td>774.4374</td>\n",
       "      <td>68.86</td>\n",
       "      <td>1.962780</td>\n",
       "      <td>2.067850</td>\n",
       "      <td>3.08253</td>\n",
       "      <td>-416.674828</td>\n",
       "      <td>-416.637913</td>\n",
       "      <td>2.5282</td>\n",
       "      <td>0.1264</td>\n",
       "      <td>-416.644949</td>\n",
       "      <td>-0.0779</td>\n",
       "      <td>-0.2042</td>\n",
       "      <td>-416.638857</td>\n",
       "      <td>0.113135</td>\n",
       "      <td>26.158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133840</th>\n",
       "      <td>837.2883</td>\n",
       "      <td>77.86</td>\n",
       "      <td>1.789470</td>\n",
       "      <td>1.939090</td>\n",
       "      <td>3.40195</td>\n",
       "      <td>-384.546083</td>\n",
       "      <td>-384.506999</td>\n",
       "      <td>1.5564</td>\n",
       "      <td>0.1066</td>\n",
       "      <td>-384.515057</td>\n",
       "      <td>-0.0845</td>\n",
       "      <td>-0.1911</td>\n",
       "      <td>-384.507943</td>\n",
       "      <td>0.134397</td>\n",
       "      <td>28.855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133847</th>\n",
       "      <td>859.4671</td>\n",
       "      <td>75.66</td>\n",
       "      <td>1.686060</td>\n",
       "      <td>1.860330</td>\n",
       "      <td>3.70013</td>\n",
       "      <td>-380.751987</td>\n",
       "      <td>-380.713968</td>\n",
       "      <td>3.0732</td>\n",
       "      <td>0.1201</td>\n",
       "      <td>-380.721537</td>\n",
       "      <td>-0.0821</td>\n",
       "      <td>-0.2022</td>\n",
       "      <td>-380.714912</td>\n",
       "      <td>0.136508</td>\n",
       "      <td>27.664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133849</th>\n",
       "      <td>850.5781</td>\n",
       "      <td>80.36</td>\n",
       "      <td>1.816570</td>\n",
       "      <td>1.858330</td>\n",
       "      <td>3.68081</td>\n",
       "      <td>-364.711052</td>\n",
       "      <td>-364.672118</td>\n",
       "      <td>3.0746</td>\n",
       "      <td>0.1805</td>\n",
       "      <td>-364.680114</td>\n",
       "      <td>-0.0059</td>\n",
       "      <td>-0.1864</td>\n",
       "      <td>-364.673062</td>\n",
       "      <td>0.147665</td>\n",
       "      <td>28.864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133852</th>\n",
       "      <td>806.6963</td>\n",
       "      <td>68.51</td>\n",
       "      <td>1.715410</td>\n",
       "      <td>1.886730</td>\n",
       "      <td>3.76654</td>\n",
       "      <td>-416.672291</td>\n",
       "      <td>-416.635254</td>\n",
       "      <td>2.8744</td>\n",
       "      <td>0.1198</td>\n",
       "      <td>-416.642338</td>\n",
       "      <td>-0.0889</td>\n",
       "      <td>-0.2087</td>\n",
       "      <td>-416.636198</td>\n",
       "      <td>0.113228</td>\n",
       "      <td>26.155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133856</th>\n",
       "      <td>971.5360</td>\n",
       "      <td>85.67</td>\n",
       "      <td>1.412820</td>\n",
       "      <td>1.656720</td>\n",
       "      <td>3.60121</td>\n",
       "      <td>-348.639441</td>\n",
       "      <td>-348.600084</td>\n",
       "      <td>1.5261</td>\n",
       "      <td>0.1498</td>\n",
       "      <td>-348.608360</td>\n",
       "      <td>-0.0515</td>\n",
       "      <td>-0.2014</td>\n",
       "      <td>-348.601028</td>\n",
       "      <td>0.158963</td>\n",
       "      <td>30.453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133858</th>\n",
       "      <td>927.4789</td>\n",
       "      <td>82.97</td>\n",
       "      <td>1.472840</td>\n",
       "      <td>1.740150</td>\n",
       "      <td>3.63247</td>\n",
       "      <td>-364.670376</td>\n",
       "      <td>-364.631314</td>\n",
       "      <td>0.8183</td>\n",
       "      <td>0.1494</td>\n",
       "      <td>-364.639427</td>\n",
       "      <td>-0.0550</td>\n",
       "      <td>-0.2044</td>\n",
       "      <td>-364.632258</td>\n",
       "      <td>0.147108</td>\n",
       "      <td>29.576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133860</th>\n",
       "      <td>1060.6837</td>\n",
       "      <td>79.94</td>\n",
       "      <td>1.262920</td>\n",
       "      <td>1.442380</td>\n",
       "      <td>3.56693</td>\n",
       "      <td>-403.070528</td>\n",
       "      <td>-403.030174</td>\n",
       "      <td>1.5690</td>\n",
       "      <td>0.3056</td>\n",
       "      <td>-403.038700</td>\n",
       "      <td>0.0828</td>\n",
       "      <td>-0.2228</td>\n",
       "      <td>-403.031118</td>\n",
       "      <td>0.172342</td>\n",
       "      <td>30.511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133861</th>\n",
       "      <td>1018.8859</td>\n",
       "      <td>80.93</td>\n",
       "      <td>1.413640</td>\n",
       "      <td>1.646100</td>\n",
       "      <td>2.70455</td>\n",
       "      <td>-403.037739</td>\n",
       "      <td>-402.997441</td>\n",
       "      <td>2.7921</td>\n",
       "      <td>0.2929</td>\n",
       "      <td>-403.006103</td>\n",
       "      <td>0.0723</td>\n",
       "      <td>-0.2206</td>\n",
       "      <td>-402.998386</td>\n",
       "      <td>0.171040</td>\n",
       "      <td>31.855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133862</th>\n",
       "      <td>995.0389</td>\n",
       "      <td>91.34</td>\n",
       "      <td>1.360020</td>\n",
       "      <td>2.001510</td>\n",
       "      <td>2.74553</td>\n",
       "      <td>-349.822503</td>\n",
       "      <td>-349.782124</td>\n",
       "      <td>0.4506</td>\n",
       "      <td>0.1856</td>\n",
       "      <td>-349.790689</td>\n",
       "      <td>-0.0152</td>\n",
       "      <td>-0.2008</td>\n",
       "      <td>-349.783068</td>\n",
       "      <td>0.183006</td>\n",
       "      <td>31.336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133866</th>\n",
       "      <td>846.5551</td>\n",
       "      <td>78.33</td>\n",
       "      <td>1.911430</td>\n",
       "      <td>1.957720</td>\n",
       "      <td>3.12173</td>\n",
       "      <td>-385.855796</td>\n",
       "      <td>-385.819470</td>\n",
       "      <td>1.4371</td>\n",
       "      <td>0.3291</td>\n",
       "      <td>-385.826182</td>\n",
       "      <td>0.0942</td>\n",
       "      <td>-0.2350</td>\n",
       "      <td>-385.820415</td>\n",
       "      <td>0.163548</td>\n",
       "      <td>26.177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133867</th>\n",
       "      <td>895.6742</td>\n",
       "      <td>85.74</td>\n",
       "      <td>1.844890</td>\n",
       "      <td>1.895490</td>\n",
       "      <td>3.01698</td>\n",
       "      <td>-349.931420</td>\n",
       "      <td>-349.894762</td>\n",
       "      <td>0.2864</td>\n",
       "      <td>0.3384</td>\n",
       "      <td>-349.901681</td>\n",
       "      <td>0.0920</td>\n",
       "      <td>-0.2465</td>\n",
       "      <td>-349.895706</td>\n",
       "      <td>0.187833</td>\n",
       "      <td>27.489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133868</th>\n",
       "      <td>849.7933</td>\n",
       "      <td>77.78</td>\n",
       "      <td>1.893160</td>\n",
       "      <td>1.965800</td>\n",
       "      <td>3.09994</td>\n",
       "      <td>-385.844608</td>\n",
       "      <td>-385.808262</td>\n",
       "      <td>1.4470</td>\n",
       "      <td>0.3287</td>\n",
       "      <td>-385.814978</td>\n",
       "      <td>0.0811</td>\n",
       "      <td>-0.2476</td>\n",
       "      <td>-385.809206</td>\n",
       "      <td>0.163634</td>\n",
       "      <td>26.226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133869</th>\n",
       "      <td>801.6841</td>\n",
       "      <td>70.28</td>\n",
       "      <td>1.960070</td>\n",
       "      <td>2.024540</td>\n",
       "      <td>3.22990</td>\n",
       "      <td>-421.770998</td>\n",
       "      <td>-421.735001</td>\n",
       "      <td>1.7397</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>-421.741500</td>\n",
       "      <td>0.0710</td>\n",
       "      <td>-0.2505</td>\n",
       "      <td>-421.735945</td>\n",
       "      <td>0.139275</td>\n",
       "      <td>24.855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133870</th>\n",
       "      <td>798.0812</td>\n",
       "      <td>70.50</td>\n",
       "      <td>1.981900</td>\n",
       "      <td>2.022870</td>\n",
       "      <td>3.23767</td>\n",
       "      <td>-421.784075</td>\n",
       "      <td>-421.748195</td>\n",
       "      <td>2.1011</td>\n",
       "      <td>0.3344</td>\n",
       "      <td>-421.754627</td>\n",
       "      <td>0.0836</td>\n",
       "      <td>-0.2508</td>\n",
       "      <td>-421.749139</td>\n",
       "      <td>0.139644</td>\n",
       "      <td>24.583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133871</th>\n",
       "      <td>817.6336</td>\n",
       "      <td>75.03</td>\n",
       "      <td>1.966220</td>\n",
       "      <td>2.015950</td>\n",
       "      <td>3.17344</td>\n",
       "      <td>-401.888757</td>\n",
       "      <td>-401.852549</td>\n",
       "      <td>0.8088</td>\n",
       "      <td>0.2988</td>\n",
       "      <td>-401.859197</td>\n",
       "      <td>0.0880</td>\n",
       "      <td>-0.2108</td>\n",
       "      <td>-401.853493</td>\n",
       "      <td>0.151201</td>\n",
       "      <td>25.714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133872</th>\n",
       "      <td>871.4877</td>\n",
       "      <td>81.97</td>\n",
       "      <td>1.875260</td>\n",
       "      <td>1.913100</td>\n",
       "      <td>3.08572</td>\n",
       "      <td>-365.952493</td>\n",
       "      <td>-365.915897</td>\n",
       "      <td>1.9005</td>\n",
       "      <td>0.2910</td>\n",
       "      <td>-365.922771</td>\n",
       "      <td>0.0824</td>\n",
       "      <td>-0.2086</td>\n",
       "      <td>-365.916841</td>\n",
       "      <td>0.175911</td>\n",
       "      <td>26.965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133873</th>\n",
       "      <td>827.7783</td>\n",
       "      <td>74.29</td>\n",
       "      <td>1.911880</td>\n",
       "      <td>1.977510</td>\n",
       "      <td>3.17845</td>\n",
       "      <td>-401.865555</td>\n",
       "      <td>-401.829217</td>\n",
       "      <td>0.6916</td>\n",
       "      <td>0.2906</td>\n",
       "      <td>-401.835917</td>\n",
       "      <td>0.0641</td>\n",
       "      <td>-0.2264</td>\n",
       "      <td>-401.830161</td>\n",
       "      <td>0.151586</td>\n",
       "      <td>25.762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133874</th>\n",
       "      <td>785.0566</td>\n",
       "      <td>73.27</td>\n",
       "      <td>1.876170</td>\n",
       "      <td>2.150950</td>\n",
       "      <td>3.51898</td>\n",
       "      <td>-384.625639</td>\n",
       "      <td>-384.590076</td>\n",
       "      <td>2.3177</td>\n",
       "      <td>0.3027</td>\n",
       "      <td>-384.596376</td>\n",
       "      <td>0.0744</td>\n",
       "      <td>-0.2283</td>\n",
       "      <td>-384.591020</td>\n",
       "      <td>0.139289</td>\n",
       "      <td>24.482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133875</th>\n",
       "      <td>802.5517</td>\n",
       "      <td>77.39</td>\n",
       "      <td>1.874250</td>\n",
       "      <td>2.113250</td>\n",
       "      <td>3.59419</td>\n",
       "      <td>-364.761334</td>\n",
       "      <td>-364.725775</td>\n",
       "      <td>1.5614</td>\n",
       "      <td>0.3224</td>\n",
       "      <td>-364.732075</td>\n",
       "      <td>0.0878</td>\n",
       "      <td>-0.2346</td>\n",
       "      <td>-364.726719</td>\n",
       "      <td>0.152397</td>\n",
       "      <td>24.517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133876</th>\n",
       "      <td>825.8430</td>\n",
       "      <td>81.37</td>\n",
       "      <td>1.844500</td>\n",
       "      <td>2.118880</td>\n",
       "      <td>3.45974</td>\n",
       "      <td>-348.726069</td>\n",
       "      <td>-348.690357</td>\n",
       "      <td>0.2462</td>\n",
       "      <td>0.3523</td>\n",
       "      <td>-348.696763</td>\n",
       "      <td>0.1023</td>\n",
       "      <td>-0.2499</td>\n",
       "      <td>-348.691301</td>\n",
       "      <td>0.164037</td>\n",
       "      <td>25.376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133877</th>\n",
       "      <td>800.3447</td>\n",
       "      <td>77.35</td>\n",
       "      <td>1.874680</td>\n",
       "      <td>2.169980</td>\n",
       "      <td>3.53569</td>\n",
       "      <td>-364.767136</td>\n",
       "      <td>-364.731554</td>\n",
       "      <td>1.6747</td>\n",
       "      <td>0.3239</td>\n",
       "      <td>-364.737867</td>\n",
       "      <td>0.0938</td>\n",
       "      <td>-0.2301</td>\n",
       "      <td>-364.732498</td>\n",
       "      <td>0.152293</td>\n",
       "      <td>24.515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133878</th>\n",
       "      <td>796.9713</td>\n",
       "      <td>77.63</td>\n",
       "      <td>1.902520</td>\n",
       "      <td>2.185850</td>\n",
       "      <td>3.52243</td>\n",
       "      <td>-364.762857</td>\n",
       "      <td>-364.727276</td>\n",
       "      <td>1.4529</td>\n",
       "      <td>0.3206</td>\n",
       "      <td>-364.733608</td>\n",
       "      <td>0.0977</td>\n",
       "      <td>-0.2229</td>\n",
       "      <td>-364.728220</td>\n",
       "      <td>0.151939</td>\n",
       "      <td>24.820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133879</th>\n",
       "      <td>777.3913</td>\n",
       "      <td>73.24</td>\n",
       "      <td>1.920670</td>\n",
       "      <td>2.195510</td>\n",
       "      <td>3.56068</td>\n",
       "      <td>-384.638652</td>\n",
       "      <td>-384.603362</td>\n",
       "      <td>1.3774</td>\n",
       "      <td>0.3331</td>\n",
       "      <td>-384.609506</td>\n",
       "      <td>0.0904</td>\n",
       "      <td>-0.2427</td>\n",
       "      <td>-384.604306</td>\n",
       "      <td>0.139809</td>\n",
       "      <td>23.968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133880</th>\n",
       "      <td>760.7472</td>\n",
       "      <td>69.37</td>\n",
       "      <td>1.904230</td>\n",
       "      <td>2.198990</td>\n",
       "      <td>3.59483</td>\n",
       "      <td>-400.663098</td>\n",
       "      <td>-400.627654</td>\n",
       "      <td>1.6637</td>\n",
       "      <td>0.2842</td>\n",
       "      <td>-400.633868</td>\n",
       "      <td>0.0588</td>\n",
       "      <td>-0.2254</td>\n",
       "      <td>-400.628599</td>\n",
       "      <td>0.127406</td>\n",
       "      <td>23.658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133881</th>\n",
       "      <td>762.6354</td>\n",
       "      <td>69.52</td>\n",
       "      <td>1.904390</td>\n",
       "      <td>2.142370</td>\n",
       "      <td>3.65648</td>\n",
       "      <td>-400.658942</td>\n",
       "      <td>-400.623500</td>\n",
       "      <td>1.2976</td>\n",
       "      <td>0.3002</td>\n",
       "      <td>-400.629713</td>\n",
       "      <td>0.0608</td>\n",
       "      <td>-0.2393</td>\n",
       "      <td>-400.624444</td>\n",
       "      <td>0.127495</td>\n",
       "      <td>23.697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133882</th>\n",
       "      <td>780.3553</td>\n",
       "      <td>73.60</td>\n",
       "      <td>1.895010</td>\n",
       "      <td>2.143140</td>\n",
       "      <td>3.67118</td>\n",
       "      <td>-380.783148</td>\n",
       "      <td>-380.747675</td>\n",
       "      <td>1.2480</td>\n",
       "      <td>0.2953</td>\n",
       "      <td>-380.753918</td>\n",
       "      <td>0.0720</td>\n",
       "      <td>-0.2233</td>\n",
       "      <td>-380.748619</td>\n",
       "      <td>0.140458</td>\n",
       "      <td>23.972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133883</th>\n",
       "      <td>803.1904</td>\n",
       "      <td>77.40</td>\n",
       "      <td>1.865820</td>\n",
       "      <td>2.151310</td>\n",
       "      <td>3.52845</td>\n",
       "      <td>-364.749650</td>\n",
       "      <td>-364.714030</td>\n",
       "      <td>1.9576</td>\n",
       "      <td>0.3003</td>\n",
       "      <td>-364.720374</td>\n",
       "      <td>0.0881</td>\n",
       "      <td>-0.2122</td>\n",
       "      <td>-364.714974</td>\n",
       "      <td>0.152222</td>\n",
       "      <td>24.796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133884</th>\n",
       "      <td>756.3557</td>\n",
       "      <td>69.48</td>\n",
       "      <td>1.937930</td>\n",
       "      <td>2.217640</td>\n",
       "      <td>3.64015</td>\n",
       "      <td>-400.662186</td>\n",
       "      <td>-400.626948</td>\n",
       "      <td>0.8626</td>\n",
       "      <td>0.3058</td>\n",
       "      <td>-400.633052</td>\n",
       "      <td>0.0742</td>\n",
       "      <td>-0.2316</td>\n",
       "      <td>-400.627892</td>\n",
       "      <td>0.127862</td>\n",
       "      <td>23.434</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>130831 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        spatialSize  polar           C           B          A           G  \\\n",
       "0           35.3641  13.21  157.706990  157.709970  157.71180  -40.498597   \n",
       "1           26.1563   9.46  191.393970  293.541110  293.60975  -56.544961   \n",
       "2           19.0002   6.31  282.945450  437.903860  799.58812  -76.422349   \n",
       "3           59.5248  16.28   35.610036   35.610036    0.00000  -77.327429   \n",
       "4           48.7476  12.99   44.593883   44.593883    0.00000  -93.431246   \n",
       "5           59.9891  14.18   34.298920   38.982300  285.48839 -114.505268   \n",
       "6          109.5031  23.95   19.906330   19.906490   80.46225  -79.787269   \n",
       "7           83.7940  16.97   23.978720   24.858720  127.83497 -115.701876   \n",
       "8          177.1963  28.78    8.593210    8.593230  160.28041 -116.633775   \n",
       "9          160.7223  24.45    9.223240    9.223270  159.03567 -132.742149   \n",
       "10         166.9728  25.11    9.073680   10.111220   57.22434 -153.812518   \n",
       "11         145.3078  21.57    9.836390   11.347930   73.84720 -169.885594   \n",
       "12         227.1361  34.75    7.420760    8.377010   29.45018 -119.078157   \n",
       "13         193.1659  27.87    8.149700    9.368600   35.09545 -154.998148   \n",
       "14         187.1015  28.13    8.903530   10.040330   39.37691 -154.985747   \n",
       "15         155.8145  30.82   12.589100   20.153020   20.15852 -117.849087   \n",
       "16         129.8910  24.04   14.180660   22.121090   25.77385 -153.766642   \n",
       "17         292.4367  35.53    4.896150    8.490110   10.12193 -193.116476   \n",
       "18         267.6148  31.83    5.149770    9.275090   10.78886 -209.187468   \n",
       "19         244.2308  28.07    5.420910   10.363880   11.14750 -225.247724   \n",
       "20         355.0621  45.46    4.486680    7.748470    7.75166 -158.370016   \n",
       "21         318.3721  38.58    4.755420    8.005680    8.67553 -194.294663   \n",
       "22         278.6264  38.52    4.425973    4.425973    0.00000 -153.482621   \n",
       "23         260.1896  32.66    4.579322    4.579322    0.00000 -169.581024   \n",
       "24         242.9308  27.70    4.732690    4.732690    0.00000 -185.667652   \n",
       "25         268.3921  31.14    4.515860    4.834500   68.51710 -190.650543   \n",
       "26         251.0007  26.25    4.664130    5.008230   67.88408 -206.747625   \n",
       "27         266.8164  26.12    4.400880    4.774410   56.25048 -227.825074   \n",
       "28         400.2236  42.32    3.368800    3.368800   80.28766 -155.937641   \n",
       "29         333.9589  40.09    4.083420    4.530050   27.39459 -155.924226   \n",
       "...             ...    ...         ...         ...        ...         ...   \n",
       "133830     832.1811  80.36    1.899860    2.064040    3.14041 -364.709768   \n",
       "133834     774.4374  68.86    1.962780    2.067850    3.08253 -416.674828   \n",
       "133840     837.2883  77.86    1.789470    1.939090    3.40195 -384.546083   \n",
       "133847     859.4671  75.66    1.686060    1.860330    3.70013 -380.751987   \n",
       "133849     850.5781  80.36    1.816570    1.858330    3.68081 -364.711052   \n",
       "133852     806.6963  68.51    1.715410    1.886730    3.76654 -416.672291   \n",
       "133856     971.5360  85.67    1.412820    1.656720    3.60121 -348.639441   \n",
       "133858     927.4789  82.97    1.472840    1.740150    3.63247 -364.670376   \n",
       "133860    1060.6837  79.94    1.262920    1.442380    3.56693 -403.070528   \n",
       "133861    1018.8859  80.93    1.413640    1.646100    2.70455 -403.037739   \n",
       "133862     995.0389  91.34    1.360020    2.001510    2.74553 -349.822503   \n",
       "133866     846.5551  78.33    1.911430    1.957720    3.12173 -385.855796   \n",
       "133867     895.6742  85.74    1.844890    1.895490    3.01698 -349.931420   \n",
       "133868     849.7933  77.78    1.893160    1.965800    3.09994 -385.844608   \n",
       "133869     801.6841  70.28    1.960070    2.024540    3.22990 -421.770998   \n",
       "133870     798.0812  70.50    1.981900    2.022870    3.23767 -421.784075   \n",
       "133871     817.6336  75.03    1.966220    2.015950    3.17344 -401.888757   \n",
       "133872     871.4877  81.97    1.875260    1.913100    3.08572 -365.952493   \n",
       "133873     827.7783  74.29    1.911880    1.977510    3.17845 -401.865555   \n",
       "133874     785.0566  73.27    1.876170    2.150950    3.51898 -384.625639   \n",
       "133875     802.5517  77.39    1.874250    2.113250    3.59419 -364.761334   \n",
       "133876     825.8430  81.37    1.844500    2.118880    3.45974 -348.726069   \n",
       "133877     800.3447  77.35    1.874680    2.169980    3.53569 -364.767136   \n",
       "133878     796.9713  77.63    1.902520    2.185850    3.52243 -364.762857   \n",
       "133879     777.3913  73.24    1.920670    2.195510    3.56068 -384.638652   \n",
       "133880     760.7472  69.37    1.904230    2.198990    3.59483 -400.663098   \n",
       "133881     762.6354  69.52    1.904390    2.142370    3.65648 -400.658942   \n",
       "133882     780.3553  73.60    1.895010    2.143140    3.67118 -380.783148   \n",
       "133883     803.1904  77.40    1.865820    2.151310    3.52845 -364.749650   \n",
       "133884     756.3557  69.48    1.937930    2.217640    3.64015 -400.662186   \n",
       "\n",
       "                 H  dipole     gap          U0    lumo    homo           U  \\\n",
       "0       -40.475117  0.0000  0.5048  -40.478930  0.1171 -0.3877  -40.476062   \n",
       "1       -56.522082  1.6256  0.3399  -56.525887  0.0829 -0.2570  -56.523026   \n",
       "2       -76.400922  1.8511  0.3615  -76.404702  0.0687 -0.2928  -76.401867   \n",
       "3       -77.304583  0.0000  0.3351  -77.308427  0.0506 -0.2845  -77.305527   \n",
       "4       -93.408425  2.8937  0.3796  -93.411888  0.0191 -0.3604  -93.409370   \n",
       "5      -114.479802  2.1089  0.2263 -114.483613 -0.0406 -0.2670 -114.480746   \n",
       "6       -79.759722  0.0000  0.4426  -79.764152  0.1041 -0.3385  -79.760666   \n",
       "7      -115.674872  1.5258  0.3437 -115.679136  0.0784 -0.2653 -115.675816   \n",
       "8      -116.604606  0.7156  0.3222 -116.609549  0.0613 -0.2609 -116.605550   \n",
       "9      -132.713619  3.8266  0.3640 -132.718150  0.0376 -0.3264 -132.714563   \n",
       "10     -153.782784  2.5682  0.2342 -153.787612 -0.0198 -0.2540 -153.783728   \n",
       "11     -169.855958  3.7286  0.2845 -169.860788  0.0302 -0.2543 -169.856903   \n",
       "12     -119.046983  0.0597  0.4179 -119.052475  0.0949 -0.3230 -119.047927   \n",
       "13     -154.967467  1.4131  0.3417 -154.972731  0.0798 -0.2619 -154.968412   \n",
       "14     -154.955100  1.1502  0.3435 -154.960361  0.0910 -0.2525 -154.956045   \n",
       "15     -117.820482  0.0005  0.3930 -117.824798  0.1042 -0.2888 -117.821426   \n",
       "16     -153.738466  1.7675  0.3724 -153.742562  0.1042 -0.2682 -153.739410   \n",
       "17     -193.082024  2.7362  0.2344 -193.088340 -0.0087 -0.2431 -193.082969   \n",
       "18     -209.153076  3.6367  0.2783 -209.159302  0.0347 -0.2436 -209.154020   \n",
       "19     -225.216131  3.4869  0.3051 -225.221461  0.0556 -0.2495 -225.217075   \n",
       "20     -158.335658  0.0897  0.4010 -158.342346  0.0843 -0.3167 -158.336603   \n",
       "21     -194.260804  1.4259  0.3351 -194.267232  0.0740 -0.2612 -194.261748   \n",
       "22     -153.454498  0.0000  0.2386 -153.459846 -0.0214 -0.2599 -153.455442   \n",
       "23     -169.552820  3.7920  0.2559 -169.557758 -0.0543 -0.3102 -169.553764   \n",
       "24     -185.643880  0.0023  0.2770 -185.648533 -0.0926 -0.3696 -185.644825   \n",
       "25     -190.619419  2.7824  0.2042 -190.624631 -0.0735 -0.2777 -190.620363   \n",
       "26     -206.716931  2.3112  0.2066 -206.721858 -0.1100 -0.3166 -206.717875   \n",
       "27     -227.793626  0.0020  0.1555 -227.798785 -0.1113 -0.2668 -227.794570   \n",
       "28     -155.902236  0.0000  0.3096 -155.908941  0.0684 -0.2412 -155.903180   \n",
       "29     -155.891347  0.7067  0.3157 -155.897345  0.0566 -0.2592 -155.892291   \n",
       "...            ...     ...     ...         ...     ...     ...         ...   \n",
       "133830 -364.671183  3.7345  0.1985 -364.679063  0.0134 -0.1852 -364.672127   \n",
       "133834 -416.637913  2.5282  0.1264 -416.644949 -0.0779 -0.2042 -416.638857   \n",
       "133840 -384.506999  1.5564  0.1066 -384.515057 -0.0845 -0.1911 -384.507943   \n",
       "133847 -380.713968  3.0732  0.1201 -380.721537 -0.0821 -0.2022 -380.714912   \n",
       "133849 -364.672118  3.0746  0.1805 -364.680114 -0.0059 -0.1864 -364.673062   \n",
       "133852 -416.635254  2.8744  0.1198 -416.642338 -0.0889 -0.2087 -416.636198   \n",
       "133856 -348.600084  1.5261  0.1498 -348.608360 -0.0515 -0.2014 -348.601028   \n",
       "133858 -364.631314  0.8183  0.1494 -364.639427 -0.0550 -0.2044 -364.632258   \n",
       "133860 -403.030174  1.5690  0.3056 -403.038700  0.0828 -0.2228 -403.031118   \n",
       "133861 -402.997441  2.7921  0.2929 -403.006103  0.0723 -0.2206 -402.998386   \n",
       "133862 -349.782124  0.4506  0.1856 -349.790689 -0.0152 -0.2008 -349.783068   \n",
       "133866 -385.819470  1.4371  0.3291 -385.826182  0.0942 -0.2350 -385.820415   \n",
       "133867 -349.894762  0.2864  0.3384 -349.901681  0.0920 -0.2465 -349.895706   \n",
       "133868 -385.808262  1.4470  0.3287 -385.814978  0.0811 -0.2476 -385.809206   \n",
       "133869 -421.735001  1.7397  0.3215 -421.741500  0.0710 -0.2505 -421.735945   \n",
       "133870 -421.748195  2.1011  0.3344 -421.754627  0.0836 -0.2508 -421.749139   \n",
       "133871 -401.852549  0.8088  0.2988 -401.859197  0.0880 -0.2108 -401.853493   \n",
       "133872 -365.915897  1.9005  0.2910 -365.922771  0.0824 -0.2086 -365.916841   \n",
       "133873 -401.829217  0.6916  0.2906 -401.835917  0.0641 -0.2264 -401.830161   \n",
       "133874 -384.590076  2.3177  0.3027 -384.596376  0.0744 -0.2283 -384.591020   \n",
       "133875 -364.725775  1.5614  0.3224 -364.732075  0.0878 -0.2346 -364.726719   \n",
       "133876 -348.690357  0.2462  0.3523 -348.696763  0.1023 -0.2499 -348.691301   \n",
       "133877 -364.731554  1.6747  0.3239 -364.737867  0.0938 -0.2301 -364.732498   \n",
       "133878 -364.727276  1.4529  0.3206 -364.733608  0.0977 -0.2229 -364.728220   \n",
       "133879 -384.603362  1.3774  0.3331 -384.609506  0.0904 -0.2427 -384.604306   \n",
       "133880 -400.627654  1.6637  0.2842 -400.633868  0.0588 -0.2254 -400.628599   \n",
       "133881 -400.623500  1.2976  0.3002 -400.629713  0.0608 -0.2393 -400.624444   \n",
       "133882 -380.747675  1.2480  0.2953 -380.753918  0.0720 -0.2233 -380.748619   \n",
       "133883 -364.714030  1.9576  0.3003 -364.720374  0.0881 -0.2122 -364.714974   \n",
       "133884 -400.626948  0.8626  0.3058 -400.633052  0.0742 -0.2316 -400.627892   \n",
       "\n",
       "             zpe      Cv  \n",
       "0       0.044749   6.469  \n",
       "1       0.034358   6.316  \n",
       "2       0.021375   6.002  \n",
       "3       0.026841   8.574  \n",
       "4       0.016601   6.278  \n",
       "5       0.026603   6.413  \n",
       "6       0.074542  10.098  \n",
       "7       0.051208   8.751  \n",
       "8       0.055410  12.482  \n",
       "9       0.045286  10.287  \n",
       "10      0.055355  11.219  \n",
       "11      0.045279  10.890  \n",
       "12      0.103182  14.840  \n",
       "13      0.079754  13.546  \n",
       "14      0.079534  12.934  \n",
       "15      0.081231  11.041  \n",
       "16      0.057289   9.176  \n",
       "17      0.083382  16.893  \n",
       "18      0.073190  16.561  \n",
       "19      0.063824  15.292  \n",
       "20      0.131146  20.273  \n",
       "21      0.107673  19.052  \n",
       "22      0.037354  15.312  \n",
       "23      0.027259  12.930  \n",
       "24      0.015951  10.398  \n",
       "25      0.037208  13.049  \n",
       "26      0.026540  11.329  \n",
       "27      0.036943  12.147  \n",
       "28      0.083896  17.447  \n",
       "29      0.084338  17.130  \n",
       "...          ...     ...  \n",
       "133830  0.147709  28.778  \n",
       "133834  0.113135  26.158  \n",
       "133840  0.134397  28.855  \n",
       "133847  0.136508  27.664  \n",
       "133849  0.147665  28.864  \n",
       "133852  0.113228  26.155  \n",
       "133856  0.158963  30.453  \n",
       "133858  0.147108  29.576  \n",
       "133860  0.172342  30.511  \n",
       "133861  0.171040  31.855  \n",
       "133862  0.183006  31.336  \n",
       "133866  0.163548  26.177  \n",
       "133867  0.187833  27.489  \n",
       "133868  0.163634  26.226  \n",
       "133869  0.139275  24.855  \n",
       "133870  0.139644  24.583  \n",
       "133871  0.151201  25.714  \n",
       "133872  0.175911  26.965  \n",
       "133873  0.151586  25.762  \n",
       "133874  0.139289  24.482  \n",
       "133875  0.152397  24.517  \n",
       "133876  0.164037  25.376  \n",
       "133877  0.152293  24.515  \n",
       "133878  0.151939  24.820  \n",
       "133879  0.139809  23.968  \n",
       "133880  0.127406  23.658  \n",
       "133881  0.127495  23.697  \n",
       "133882  0.140458  23.972  \n",
       "133883  0.152222  24.796  \n",
       "133884  0.127862  23.434  \n",
       "\n",
       "[130831 rows x 15 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[list_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# 2. feature enigneering (BoB)\n",
    "fit_list = get_attributes(np.array(df['atomList']), np.array(df['atomCoords']))\n",
    "\n",
    "feature_methods = [CoulombMatrix(), BagOfBonds()]\n",
    "\n",
    "feat_co_bob = feature_methods[1]\n",
    "\n",
    "train_list_bob = feat_co_bob.fit_transform(fit_list)\n",
    "\n",
    "feat_co_cm = feature_methods[0]\n",
    "\n",
    "train_list_cm = feat_co_cm.fit_transform(fit_list)\n",
    "\n",
    "#train_list_scale = MinMaxScaler().fit_transform(train_list)\n",
    "\n",
    "#train_list_scale_abs = MaxAbsScaler().fit_transform(train_list)\n",
    "\n",
    "#feature_list, target_list = get_train_vali_test(train_list_scale_abs, df['list_all'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(666,)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(train_list_bob[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "841"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(feature_list_cm[0])[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train_list_scale = MinMaxScaler().fit_transform(train_list_cm)\n",
    "\n",
    "train_list_scale_abs_cm = MaxAbsScaler().fit_transform(train_list_cm)\n",
    "\n",
    "feature_list_cm, target_list = get_train_vali_test(train_list_scale_abs_cm, df['gap'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dic_features = {}\n",
    "\n",
    "dic_features['bob_features']=train_list_bob\n",
    "#dic_features['cm_features']=train_list_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dic_features['targets']= df[list_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_bob = list(train_list_bob)\n",
    "list_cm = list(train_list_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['bob']=list_bob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['cm']= list_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_pickle('df_features_targets.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef train_DNN(target_single=None, model_old = None, epochs = 100):\\n    \\n    #Run1, the single-DNN on electro_list:\\n    start = timeit.default_timer()\\n\\n    #R1 generate feature and target_list:\\n    feature_list, target_list = get_train_vali_test(train_list_scale, df[target_single])\\n\\n    #R3 instance model and fit\\n    if model_old:\\n        \\n        model_1 = model_old\\n        \\n    else:\\n        model_1 = test_single_dnn()\\n\\n    history = model_1.fit(feature_list[0], target_list[0],                                    shuffle=True,\\n                nb_epoch=epochs,\\n                batch_size=100,\\n                verbose = 0,\\n               validation_data = (feature_list[1],                                    target_list[1]))\\n\\n    stop = timeit.default_timer()\\n\\n    print (\"The running takes %r min\" %((stop-start)/60))\\n    \\n    return model_1, history, feature_list, target_list\\n\\n\\ndef plot_loss(history = None):\\n    # plot the loss \\n    print(history.history.keys())\\n    #plt.plot(history.history[\\'o2_loss\\'])\\n    #plt.plot(history.history[\\'val_o2_loss\\'])\\n    plt.plot(history.history[\\'loss\\'], label = \\'loss\\')\\n    plt.plot(history.history[\\'val_loss\\'], label = \\'val_loss\\')\\n    plt.legend()\\n    plt.show()\\n    \\n    \\ndef metric_check(model_=None, feature_list=None, target_list=None):\\n    # predict results and check the predicting performance \\n    pred_target_1 = np.reshape(model_.predict(feature_list[-1]),-1)\\n    for metric in metrics:\\n        print (str(metric), metric(pred_target_1, target_list[-1]))\\n'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def train_DNN(target_single=None, model_old = None, epochs = 100):\n",
    "    \n",
    "#     #Run1, the single-DNN on electro_list:\n",
    "#     start = timeit.default_timer()\n",
    "\n",
    "#     #R1 generate feature and target_list:\n",
    "#     feature_list, target_list = get_train_vali_test(train_list_scale, df[target_single])\n",
    "\n",
    "#     #R3 instance model and fit\n",
    "#     if model_old:\n",
    "        \n",
    "#         model_1 = model_old\n",
    "        \n",
    "#     else:\n",
    "#         model_1 = test_single_dnn()\n",
    "\n",
    "#     history = model_1.fit(feature_list[0], target_list[0],\\\n",
    "#                                     shuffle=True,\n",
    "#                 nb_epoch=epochs,\n",
    "#                 batch_size=100,\n",
    "#                 verbose = 0,\n",
    "#                validation_data = (feature_list[1], \\\n",
    "#                                    target_list[1]))\n",
    "\n",
    "#     stop = timeit.default_timer()\n",
    "\n",
    "#     print (\"The running takes %r min\" %((stop-start)/60))\n",
    "    \n",
    "#     return model_1, history, feature_list, target_list\n",
    "\n",
    "\n",
    "# def plot_loss(history = None):\n",
    "#     # plot the loss \n",
    "#     print(history.history.keys())\n",
    "#     #plt.plot(history.history['o2_loss'])\n",
    "#     #plt.plot(history.history['val_o2_loss'])\n",
    "#     plt.plot(history.history['loss'], label = 'loss')\n",
    "#     plt.plot(history.history['val_loss'], label = 'val_loss')\n",
    "#     plt.legend()\n",
    "#     plt.show()\n",
    "    \n",
    "    \n",
    "# def metric_check(model_=None, feature_list=None, target_list=None):\n",
    "#     # predict results and check the predicting performance \n",
    "#     pred_target_1 = np.reshape(model_.predict(feature_list[-1]),-1)\n",
    "#     for metric in metrics:\n",
    "#         print (str(metric), metric(pred_target_1, target_list[-1]))\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''test can be deleted'''\n",
    "df_all = pd.read_pickle('df_features_targets.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_only= pd.read_pickle('df_bob_target.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "only_bob = df_only['feature_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(666,)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(only_bob[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allbob = np.array(df_all['bob'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "total size of new array must be unchanged",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-102-2d055bf0a783>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mallbob_reshape\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mallbob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mallbob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m666\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: total size of new array must be unchanged"
     ]
    }
   ],
   "source": [
    "allbob_reshape =allbob.reshape(np.shape(allbob)[0], 666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600,)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allbob[:600].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(83731, 666)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "only_bob[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(allbob[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  5.49477531,\n",
       "        5.49476946,  5.49474894,  5.49474169,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.56081483,  0.5608061 ,  0.56080599,\n",
       "        0.56080582,  0.56080321,  0.56080291,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allbob[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allbob_part1, allbob_part2 = allbob[0:100], allbob[100:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "part12 = [allbob_part1, allbob_part2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(666,)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(part12[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(part12[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "total size of new array must be unchanged",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-129-4250c0f64c6a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtry_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpart12\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m666\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: total size of new array must be unchanged"
     ]
    }
   ],
   "source": [
    "try_1 = part12[0].reshape(100,666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(part12[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>numAtoms</th>\n",
       "      <th>dbindex</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>dipole</th>\n",
       "      <th>polar</th>\n",
       "      <th>homo</th>\n",
       "      <th>lumo</th>\n",
       "      <th>gap</th>\n",
       "      <th>spatialSize</th>\n",
       "      <th>zpe</th>\n",
       "      <th>U0</th>\n",
       "      <th>U</th>\n",
       "      <th>H</th>\n",
       "      <th>G</th>\n",
       "      <th>Cv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>130831.000000</td>\n",
       "      <td>130831.000000</td>\n",
       "      <td>130831.000000</td>\n",
       "      <td>130831.000000</td>\n",
       "      <td>130831.000000</td>\n",
       "      <td>130831.000000</td>\n",
       "      <td>130831.000000</td>\n",
       "      <td>130831.000000</td>\n",
       "      <td>130831.000000</td>\n",
       "      <td>130831.000000</td>\n",
       "      <td>130831.000000</td>\n",
       "      <td>130831.000000</td>\n",
       "      <td>130831.000000</td>\n",
       "      <td>130831.000000</td>\n",
       "      <td>130831.000000</td>\n",
       "      <td>130831.000000</td>\n",
       "      <td>130831.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>18.032500</td>\n",
       "      <td>66839.584976</td>\n",
       "      <td>9.966023</td>\n",
       "      <td>1.406728</td>\n",
       "      <td>1.127399</td>\n",
       "      <td>2.672953</td>\n",
       "      <td>75.281185</td>\n",
       "      <td>-0.240210</td>\n",
       "      <td>0.011835</td>\n",
       "      <td>0.252045</td>\n",
       "      <td>1189.410643</td>\n",
       "      <td>0.149090</td>\n",
       "      <td>-410.819448</td>\n",
       "      <td>-410.810977</td>\n",
       "      <td>-410.810033</td>\n",
       "      <td>-410.852852</td>\n",
       "      <td>31.620364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.943715</td>\n",
       "      <td>38457.235392</td>\n",
       "      <td>1830.462974</td>\n",
       "      <td>1.600828</td>\n",
       "      <td>1.107471</td>\n",
       "      <td>1.503479</td>\n",
       "      <td>8.173831</td>\n",
       "      <td>0.021967</td>\n",
       "      <td>0.046850</td>\n",
       "      <td>0.047192</td>\n",
       "      <td>280.478157</td>\n",
       "      <td>0.033138</td>\n",
       "      <td>39.894282</td>\n",
       "      <td>39.894066</td>\n",
       "      <td>39.894066</td>\n",
       "      <td>39.894784</td>\n",
       "      <td>4.067581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.337120</td>\n",
       "      <td>0.331180</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.310000</td>\n",
       "      <td>-0.428600</td>\n",
       "      <td>-0.175000</td>\n",
       "      <td>0.024600</td>\n",
       "      <td>19.000200</td>\n",
       "      <td>0.015951</td>\n",
       "      <td>-714.568061</td>\n",
       "      <td>-714.560153</td>\n",
       "      <td>-714.559209</td>\n",
       "      <td>-714.602138</td>\n",
       "      <td>6.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>16.000000</td>\n",
       "      <td>33749.500000</td>\n",
       "      <td>2.555040</td>\n",
       "      <td>1.091545</td>\n",
       "      <td>0.911495</td>\n",
       "      <td>1.577800</td>\n",
       "      <td>70.480000</td>\n",
       "      <td>-0.252600</td>\n",
       "      <td>-0.023300</td>\n",
       "      <td>0.217000</td>\n",
       "      <td>1017.431250</td>\n",
       "      <td>0.125638</td>\n",
       "      <td>-437.878804</td>\n",
       "      <td>-437.870862</td>\n",
       "      <td>-437.869918</td>\n",
       "      <td>-437.911830</td>\n",
       "      <td>28.955000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>18.000000</td>\n",
       "      <td>67093.000000</td>\n",
       "      <td>3.090100</td>\n",
       "      <td>1.370650</td>\n",
       "      <td>1.082030</td>\n",
       "      <td>2.475300</td>\n",
       "      <td>75.600000</td>\n",
       "      <td>-0.241100</td>\n",
       "      <td>0.012600</td>\n",
       "      <td>0.250200</td>\n",
       "      <td>1147.221100</td>\n",
       "      <td>0.148629</td>\n",
       "      <td>-416.808462</td>\n",
       "      <td>-416.800533</td>\n",
       "      <td>-416.799589</td>\n",
       "      <td>-416.841301</td>\n",
       "      <td>31.578000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>20.000000</td>\n",
       "      <td>100063.500000</td>\n",
       "      <td>3.836890</td>\n",
       "      <td>1.655050</td>\n",
       "      <td>1.282720</td>\n",
       "      <td>3.596350</td>\n",
       "      <td>80.610000</td>\n",
       "      <td>-0.228900</td>\n",
       "      <td>0.050900</td>\n",
       "      <td>0.289400</td>\n",
       "      <td>1309.047000</td>\n",
       "      <td>0.171397</td>\n",
       "      <td>-387.040458</td>\n",
       "      <td>-387.031217</td>\n",
       "      <td>-387.030273</td>\n",
       "      <td>-387.074521</td>\n",
       "      <td>34.298000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>29.000000</td>\n",
       "      <td>133885.000000</td>\n",
       "      <td>619867.683140</td>\n",
       "      <td>437.903860</td>\n",
       "      <td>282.945450</td>\n",
       "      <td>29.556400</td>\n",
       "      <td>196.620000</td>\n",
       "      <td>-0.101700</td>\n",
       "      <td>0.193500</td>\n",
       "      <td>0.622100</td>\n",
       "      <td>3374.753200</td>\n",
       "      <td>0.273944</td>\n",
       "      <td>-40.478930</td>\n",
       "      <td>-40.476062</td>\n",
       "      <td>-40.475117</td>\n",
       "      <td>-40.498597</td>\n",
       "      <td>46.969000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            numAtoms        dbindex              A              B  \\\n",
       "count  130831.000000  130831.000000  130831.000000  130831.000000   \n",
       "mean       18.032500   66839.584976       9.966023       1.406728   \n",
       "std         2.943715   38457.235392    1830.462974       1.600828   \n",
       "min         3.000000       1.000000       0.000000       0.337120   \n",
       "25%        16.000000   33749.500000       2.555040       1.091545   \n",
       "50%        18.000000   67093.000000       3.090100       1.370650   \n",
       "75%        20.000000  100063.500000       3.836890       1.655050   \n",
       "max        29.000000  133885.000000  619867.683140     437.903860   \n",
       "\n",
       "                   C         dipole          polar           homo  \\\n",
       "count  130831.000000  130831.000000  130831.000000  130831.000000   \n",
       "mean        1.127399       2.672953      75.281185      -0.240210   \n",
       "std         1.107471       1.503479       8.173831       0.021967   \n",
       "min         0.331180       0.000000       6.310000      -0.428600   \n",
       "25%         0.911495       1.577800      70.480000      -0.252600   \n",
       "50%         1.082030       2.475300      75.600000      -0.241100   \n",
       "75%         1.282720       3.596350      80.610000      -0.228900   \n",
       "max       282.945450      29.556400     196.620000      -0.101700   \n",
       "\n",
       "                lumo            gap    spatialSize            zpe  \\\n",
       "count  130831.000000  130831.000000  130831.000000  130831.000000   \n",
       "mean        0.011835       0.252045    1189.410643       0.149090   \n",
       "std         0.046850       0.047192     280.478157       0.033138   \n",
       "min        -0.175000       0.024600      19.000200       0.015951   \n",
       "25%        -0.023300       0.217000    1017.431250       0.125638   \n",
       "50%         0.012600       0.250200    1147.221100       0.148629   \n",
       "75%         0.050900       0.289400    1309.047000       0.171397   \n",
       "max         0.193500       0.622100    3374.753200       0.273944   \n",
       "\n",
       "                  U0              U              H              G  \\\n",
       "count  130831.000000  130831.000000  130831.000000  130831.000000   \n",
       "mean     -410.819448    -410.810977    -410.810033    -410.852852   \n",
       "std        39.894282      39.894066      39.894066      39.894784   \n",
       "min      -714.568061    -714.560153    -714.559209    -714.602138   \n",
       "25%      -437.878804    -437.870862    -437.869918    -437.911830   \n",
       "50%      -416.808462    -416.800533    -416.799589    -416.841301   \n",
       "75%      -387.040458    -387.031217    -387.030273    -387.074521   \n",
       "max       -40.478930     -40.476062     -40.475117     -40.498597   \n",
       "\n",
       "                  Cv  \n",
       "count  130831.000000  \n",
       "mean       31.620364  \n",
       "std         4.067581  \n",
       "min         6.002000  \n",
       "25%        28.955000  \n",
       "50%        31.578000  \n",
       "75%        34.298000  \n",
       "max        46.969000  "
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "ERROR (theano.sandbox.cuda): ERROR: Not using GPU. Initialisation of device 0 failed:\n",
      "initCnmem: cnmemInit call failed! Reason=CNMEM_STATUS_OUT_OF_MEMORY. numdev=1\n",
      "\n",
      "ERROR (theano.sandbox.cuda): ERROR: Not using GPU. Initialisation of device gpu failed:\n",
      "initCnmem: cnmemInit call failed! Reason=CNMEM_STATUS_OUT_OF_MEMORY. numdev=1\n",
      "\n",
      "Using Theano backend.\n",
      "Using Theano backend.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'tests'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-141-1b18531e608b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_results_sum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./Results/dic_pca_base_svd.pkl'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/peng/anaconda2/lib/python2.7/site-packages/pandas/io/pickle.pyc\u001b[0m in \u001b[0;36mread_pickle\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtry_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m     \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mPY3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/peng/anaconda2/lib/python2.7/site-packages/pandas/io/pickle.pyc\u001b[0m in \u001b[0;36mtry_read\u001b[1;34m(path, encoding)\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfh\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mpc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/peng/anaconda2/lib/python2.7/site-packages/pandas/compat/pickle_compat.pyc\u001b[0m in \u001b[0;36mload\u001b[1;34m(fh, encoding, compat, is_verbose)\u001b[0m\n\u001b[0;32m    115\u001b[0m         \u001b[0mup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_verbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mis_verbose\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m     \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/peng/anaconda2/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36mload\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    862\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    863\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 864\u001b[1;33m                 \u001b[0mdispatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    865\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0m_Stop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    866\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/peng/anaconda2/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36mload_global\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1094\u001b[0m         \u001b[0mmodule\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1095\u001b[0m         \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1096\u001b[1;33m         \u001b[0mklass\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1097\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mklass\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1098\u001b[0m     \u001b[0mdispatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mGLOBAL\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_global\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/peng/anaconda2/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36mfind_class\u001b[1;34m(self, module, name)\u001b[0m\n\u001b[0;32m   1128\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfind_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1129\u001b[0m         \u001b[1;31m# Subclasses may override this\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1130\u001b[1;33m         \u001b[0m__import__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[0mmod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mklass\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/peng/anaconda2/lib/python2.7/site-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/peng/anaconda2/lib/python2.7/site-packages/keras/backend/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m_BACKEND\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'theano'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Using Theano backend.\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mtheano_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m \u001b[1;32melif\u001b[0m \u001b[0m_BACKEND\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'tensorflow'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Using TensorFlow backend.\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/peng/anaconda2/lib/python2.7/site-packages/keras/backend/theano_backend.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensor\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msandbox\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrng_mrg\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMRG_RandomStreams\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mRandomStreams\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignal\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpool\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnnet\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconv3d2d\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/peng/anaconda2/lib/python2.7/site-packages/theano/__init__.pyc\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;31m# needed during that phase.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtests\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtests\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"TheanoNoseTester\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m     \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTheanoNoseTester\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'module' object has no attribute 'tests'"
     ]
    }
   ],
   "source": [
    "df_results_sum = pd.read_pickle('./Results/dic_pca_base_svd.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
